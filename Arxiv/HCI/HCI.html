<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.9" />
    <style>
      :root {
        --c-bg: #fff;
      }
      html.dark {
        --c-bg: #22272e;
      }
      html,
      body {
        background-color: var(--c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem('vuepress-color-scheme')
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia('(prefers-color-scheme: dark)').matches
      if (userMode === 'dark' || (userMode !== 'light' && systemDarkMode)) {
        document.documentElement.classList.toggle('dark', true)
      }
    </script>
    <meta property="og:url" content="https://www.shisanyi.vip/Arxiv/HCI/HCI.html"><meta property="og:site_name" content="饰三姨——中古饰品"><meta property="og:title" content="HCI"><meta property="og:description" content="HCI 2024-05-09 A Survey on Visualization Approaches in Political Science for Social and Political Factors: Progress to Date and Future Opportunities Authors: Dongyun Han, Abdull..."><meta property="og:type" content="article"><meta property="og:locale" content="en-US"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"HCI","image":[""],"dateModified":null,"author":[]}</script><meta name="description" content="饰品美学，情绪表达"><meta name="keywords" content="饰三姨,shisanyi,jewelry,Electronic Commerce,design,vintage"><meta name="author" content="饰三姨@黄飞鸿"><script src="/javascript/translate.js"></script><script src="/javascript/model-viewer.min.js" type="module"></script><meta name="google-site-verification" content="1"><link rel="icon" href="/favicon.ico"><title>HCI | 饰三姨——中古饰品</title>
    <link rel="preload" href="/assets/style-Cgp8KAEs.css" as="style"><link rel="stylesheet" href="/assets/style-Cgp8KAEs.css">
    <link rel="modulepreload" href="/assets/app-2W_bNIRU.js"><link rel="modulepreload" href="/assets/HCI.html-B9tJAlL2.js">
    <link rel="prefetch" href="/assets/LLM.html-Vf5rr4L3.js" as="script"><link rel="prefetch" href="/assets/test.html-crTLPUCZ.js" as="script"><link rel="prefetch" href="/assets/test.html-cVV4X2xu.js" as="script"><link rel="prefetch" href="/assets/test.html-C3LWMQ-C.js" as="script"><link rel="prefetch" href="/assets/《稀缺大脑》1：注定陷入的循环.html-DyDxatlY.js" as="script"><link rel="prefetch" href="/assets/《稀缺大脑》2：浓缩的危险.html-DsQPTcQ0.js" as="script"><link rel="prefetch" href="/assets/《稀缺大脑》3：怎样跳出循环.html-BgJf2Qgv.js" as="script"><link rel="prefetch" href="/assets/养育孩子与自我成长.html-rnj8VnjC.js" as="script"><link rel="prefetch" href="/assets/test.html-C8l-ExX6.js" as="script"><link rel="prefetch" href="/assets/test.html-D7srifDh.js" as="script"><link rel="prefetch" href="/assets/test.html-CB746A7Q.js" as="script"><link rel="prefetch" href="/assets/test.html-CiTsFLmX.js" as="script"><link rel="prefetch" href="/assets/test.html-CRYj3129.js" as="script"><link rel="prefetch" href="/assets/test.html-DUI3ObKi.js" as="script"><link rel="prefetch" href="/assets/test.html-DV9HsXJH.js" as="script"><link rel="prefetch" href="/assets/404.html-CUWVwVdu.js" as="script"><link rel="prefetch" href="/assets/index.html-CfEtgRtb.js" as="script"><link rel="prefetch" href="/assets/index.html-KskMwWFW.js" as="script"><link rel="prefetch" href="/assets/index.html-DEpO5nLW.js" as="script"><link rel="prefetch" href="/assets/index.html-gQNO6oaP.js" as="script"><link rel="prefetch" href="/assets/index.html-CRll8Y4f.js" as="script"><link rel="prefetch" href="/assets/index.html-IMD8wbfI.js" as="script"><link rel="prefetch" href="/assets/index.html-D_sYGWbq.js" as="script"><link rel="prefetch" href="/assets/index.html-ujVdK9Z3.js" as="script"><link rel="prefetch" href="/assets/index.html-BVd0Mpcm.js" as="script"><link rel="prefetch" href="/assets/index.html-CUdu_sl0.js" as="script"><link rel="prefetch" href="/assets/index.html-zNprx_kr.js" as="script"><link rel="prefetch" href="/assets/index.html-BJSL61-P.js" as="script"><link rel="prefetch" href="/assets/index.html-D-ufkd6x.js" as="script"><link rel="prefetch" href="/assets/index.html-Dbedmkpw.js" as="script"><link rel="prefetch" href="/assets/index.html-CPiJvtO0.js" as="script"><link rel="prefetch" href="/assets/index.html-CcIL3bv6.js" as="script"><link rel="prefetch" href="/assets/index.html-Bn3Cs3Tb.js" as="script"><link rel="prefetch" href="/assets/index.html-CCGGgAfU.js" as="script"><link rel="prefetch" href="/assets/index.html-CeoWnT1c.js" as="script"><link rel="prefetch" href="/assets/index.html-DRWtIImZ.js" as="script"><link rel="prefetch" href="/assets/index.html-D0tBNA2o.js" as="script"><link rel="prefetch" href="/assets/index.html-_MaZhRDy.js" as="script"><link rel="prefetch" href="/assets/index.html-CrLmbDjP.js" as="script"><link rel="prefetch" href="/assets/index.html-B_jYJ_Ws.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><div class="theme-container"><!--[--><header class="navbar"><div class="toggle-sidebar-button" title="toggle sidebar" aria-expanded="false" role="button" tabindex="0"><div class="icon" aria-hidden="true"><span></span><span></span><span></span></div></div><span><a class="route-link" href="/"><img class="logo" src="/logo.png" alt="饰三姨——中古饰品"><span class="site-name can-hide" aria-hidden="true">饰三姨——中古饰品</span></a></span><div class="navbar-items-wrapper" style=""><!--[--><!--]--><nav class="navbar-items can-hide" aria-label="site navigation"><!--[--><div class="navbar-item"><a class="route-link" href="/posts/Brand/" aria-label="Brand"><!--[--><!--[--><!--]--> Brand <!--[--><!--]--><!--]--></a></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="Product"><span class="title">Product</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="Product"><span class="title">Product</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Product/Earrings/" aria-label="Earrings"><!--[--><!--[--><!--]--> Earrings <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Product/Necklaces/" aria-label="Necklaces"><!--[--><!--[--><!--]--> Necklaces <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Product/Brooches/" aria-label="Brooches"><!--[--><!--[--><!--]--> Brooches <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Product/Fingerrings/" aria-label="Fingerrings"><!--[--><!--[--><!--]--> Fingerrings <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Product/Bracelets/" aria-label="Bracelets"><!--[--><!--[--><!--]--> Bracelets <!--[--><!--]--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="Jewelry Story"><span class="title">Jewelry Story</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="Jewelry Story"><span class="title">Jewelry Story</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a class="route-link" href="/posts/JewelryStory/Historyofjewelry/" aria-label="History of jewelry"><!--[--><!--[--><!--]--> History of jewelry <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/JewelryStory/Regionofjewelry/" aria-label="Region of jewelry"><!--[--><!--[--><!--]--> Region of jewelry <!--[--><!--]--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="Lifestyle"><span class="title">Lifestyle</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="Lifestyle"><span class="title">Lifestyle</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Lifestyle/Mind&amp;Spirit/" aria-label="Mind &amp; Spirit"><!--[--><!--[--><!--]--> Mind &amp; Spirit <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Lifestyle/Operations/" aria-label="Operations"><!--[--><!--[--><!--]--> Operations <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Lifestyle/Parentchild/" aria-label="Parent child"><!--[--><!--[--><!--]--> Parent child <!--[--><!--]--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-item"><a class="route-link" href="/posts/AiOfJewelry/" aria-label="Ai Of Jewelry"><!--[--><!--[--><!--]--> Ai Of Jewelry <!--[--><!--]--><!--]--></a></div><!--]--></nav><!--[--><!--[--><select id="translateSelectLanguage"><option value="Not Translate" selected="selected">Not Translate</option><option value="Default">Auto Translate</option><option value="english">English</option><option value="chinese_simplified">简体中文</option><option value="chinese_traditional">繁體中文</option><option value="russian">Русский</option><option value="japanese">しろうと</option><option value="korean">한국어</option><option value="deutsch">Deutsch</option><option value="spanish">Español</option><option value="italian">italiano</option><option value="norwegian">Norge</option><option value="dutch">nederlands</option><option value="filipino">Pilipino</option><option value="lao">ກະຣຸນາ</option><option value="romanian">Română</option><option value="nepali">नेपालीName</option><option value="french">Français</option><option value="haitian_creole">Kreyòl ayisyen</option><option value="czech">český</option><option value="swedish">Svenska</option><option value="russian">Русский язык</option><option value="malagasy">Malagasy</option><option value="burmese">ဗာရမ်</option><option value="pashto">پښتوName</option><option value="thai">คนไทย</option><option value="armenian">Արմենյան</option><option value="persian">Persian</option><option value="kurdish">Kurdî</option><option value="turkish">Türkçe</option><option value="hindi">हिन्दी</option><option value="bulgarian">български</option><option value="malay">Malay</option><option value="swahili">Kiswahili</option><option value="oriya">ଓଡିଆ</option><option value="irish">Íris</option><option value="gujarati">ગુજરાતી</option><option value="slovak">Slovenská</option><option value="hebrew">היברית</option><option value="hungarian">magyar</option><option value="marathi">मराठीName</option><option value="tamil">தாமில்</option><option value="estonian">eesti keel</option><option value="malayalam">മലമാലം</option><option value="inuktitut">ᐃᓄᒃᑎᑐᑦ</option><option value="arabic">بالعربية</option><option value="slovene">slovenščina</option><option value="bengali">বেঙ্গালী</option><option value="urdu">اوردو</option><option value="azerbaijani">azerbaijani</option><option value="portuguese">português</option><option value="samoan">lifiava</option><option value="afrikaans">afrikaans</option><option value="greek">ελληνικά</option><option value="danish">dansk</option><option value="amharic">amharic</option><option value="albanian">albanian</option><option value="lithuanian">Lietuva</option><option value="vietnamese">Tiếng Việt</option><option value="maltese">Malti</option><option value="finnish">suomi</option><option value="catalan">català</option><option value="croatian">hrvatski</option><option value="bosnian">bosnian</option><option value="polish">Polski</option><option value="latvian">latviešu</option><option value="maori">Maori</option></select><!--]--><!--]--><button class="toggle-color-mode-button" title="toggle color mode"><svg style="" class="icon" focusable="false" viewBox="0 0 32 32"><path d="M16 12.005a4 4 0 1 1-4 4a4.005 4.005 0 0 1 4-4m0-2a6 6 0 1 0 6 6a6 6 0 0 0-6-6z" fill="currentColor"></path><path d="M5.394 6.813l1.414-1.415l3.506 3.506L8.9 10.318z" fill="currentColor"></path><path d="M2 15.005h5v2H2z" fill="currentColor"></path><path d="M5.394 25.197L8.9 21.691l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 25.005h2v5h-2z" fill="currentColor"></path><path d="M21.687 23.106l1.414-1.415l3.506 3.506l-1.414 1.414z" fill="currentColor"></path><path d="M25 15.005h5v2h-5z" fill="currentColor"></path><path d="M21.687 8.904l3.506-3.506l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 2.005h2v5h-2z" fill="currentColor"></path></svg><svg style="display:none;" class="icon" focusable="false" viewBox="0 0 32 32"><path d="M13.502 5.414a15.075 15.075 0 0 0 11.594 18.194a11.113 11.113 0 0 1-7.975 3.39c-.138 0-.278.005-.418 0a11.094 11.094 0 0 1-3.2-21.584M14.98 3a1.002 1.002 0 0 0-.175.016a13.096 13.096 0 0 0 1.825 25.981c.164.006.328 0 .49 0a13.072 13.072 0 0 0 10.703-5.555a1.01 1.01 0 0 0-.783-1.565A13.08 13.08 0 0 1 15.89 4.38A1.015 1.015 0 0 0 14.98 3z" fill="currentColor"></path></svg></button><!----></div></header><!--]--><div class="sidebar-mask"></div><!--[--><aside class="sidebar"><nav class="navbar-items" aria-label="site navigation"><!--[--><div class="navbar-item"><a class="route-link" href="/posts/Brand/" aria-label="Brand"><!--[--><!--[--><!--]--> Brand <!--[--><!--]--><!--]--></a></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="Product"><span class="title">Product</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="Product"><span class="title">Product</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Product/Earrings/" aria-label="Earrings"><!--[--><!--[--><!--]--> Earrings <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Product/Necklaces/" aria-label="Necklaces"><!--[--><!--[--><!--]--> Necklaces <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Product/Brooches/" aria-label="Brooches"><!--[--><!--[--><!--]--> Brooches <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Product/Fingerrings/" aria-label="Fingerrings"><!--[--><!--[--><!--]--> Fingerrings <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Product/Bracelets/" aria-label="Bracelets"><!--[--><!--[--><!--]--> Bracelets <!--[--><!--]--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="Jewelry Story"><span class="title">Jewelry Story</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="Jewelry Story"><span class="title">Jewelry Story</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a class="route-link" href="/posts/JewelryStory/Historyofjewelry/" aria-label="History of jewelry"><!--[--><!--[--><!--]--> History of jewelry <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/JewelryStory/Regionofjewelry/" aria-label="Region of jewelry"><!--[--><!--[--><!--]--> Region of jewelry <!--[--><!--]--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="Lifestyle"><span class="title">Lifestyle</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="Lifestyle"><span class="title">Lifestyle</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Lifestyle/Mind&amp;Spirit/" aria-label="Mind &amp; Spirit"><!--[--><!--[--><!--]--> Mind &amp; Spirit <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Lifestyle/Operations/" aria-label="Operations"><!--[--><!--[--><!--]--> Operations <!--[--><!--]--><!--]--></a></li><li class="navbar-dropdown-item"><a class="route-link" href="/posts/Lifestyle/Parentchild/" aria-label="Parent child"><!--[--><!--[--><!--]--> Parent child <!--[--><!--]--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-item"><a class="route-link" href="/posts/AiOfJewelry/" aria-label="Ai Of Jewelry"><!--[--><!--[--><!--]--> Ai Of Jewelry <!--[--><!--]--><!--]--></a></div><!--]--></nav><!--[--><!--]--><ul class="sidebar-items"><!--[--><li><p tabindex="0" class="sidebar-item sidebar-heading">HCI <!----></p><ul style="" class="sidebar-item-children"><!--[--><li><a class="route-link sidebar-item" href="#_2024-05-09" aria-label="2024-05-09"><!--[--><!--[--><!--]--> 2024-05-09 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-05-08" aria-label="2024-05-08"><!--[--><!--[--><!--]--> 2024-05-08 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-05-07" aria-label="2024-05-07"><!--[--><!--[--><!--]--> 2024-05-07 <!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link sidebar-item" href="#_2024-05-06" aria-label="2024-05-06"><!--[--><!--[--><!--]--> 2024-05-06 <!--[--><!--]--><!--]--></a><!----></li><!--]--></ul></li><!--]--></ul><!--[--><!--]--></aside><!--]--><!--[--><main class="page"><!--[--><!--]--><div class="theme-default-content"><!--[--><!--]--><div><h1 id="hci" tabindex="-1"><a class="header-anchor" href="#hci"><span>HCI</span></a></h1><h2 id="_2024-05-09" tabindex="-1"><a class="header-anchor" href="#_2024-05-09"><span>2024-05-09</span></a></h2><h4 id="a-survey-on-visualization-approaches-in-political-science-for-social-and-political-factors-progress-to-date-and-future-opportunities" tabindex="-1"><a class="header-anchor" href="#a-survey-on-visualization-approaches-in-political-science-for-social-and-political-factors-progress-to-date-and-future-opportunities"><span>A Survey on Visualization Approaches in Political Science for Social and Political Factors: Progress to Date and Future Opportunities</span></a></h4><p><strong>Authors</strong>: Dongyun Han, Abdullah-Al-Raihan Nayeem, Jason Windett, Yaoyao Dai, Benjamin Radford, Isaac Cho</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.05947v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.05947v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Politics is the set of activities related to strategic decision-making in groups. Political scientists study the strategic interactions between states, institutions, politicians, and citizens; they seek to understand the causes and consequences of those decisions and interactions. While some decisions might alleviate social problems, others might lead to disasters such as war and conflict. Data visualization approaches have the potential to assist political scientists in their studies by providing visual contexts. However, political researchers&#39; perspectives on data visualization are unclear. This paper examines political scientists&#39; perspectives on visualization and how they apply data visualization in their research. We discovered a growing trend in the use of graphs in political science journals. However, we also found a knowledge gap between the political science and visualization domains, such as effective visualization techniques for tasks and the use of color studied by visualization researchers. To reduce this gap, we survey visualization techniques applicable to the political scientists&#39; research and report the visual analytics systems implemented for and evaluated by political scientists. At the end of this paper, we present an outline of future opportunities, including research topics and methodologies, for multidisciplinary research in political science and data analytics. Through this paper, we expect visualization researchers to get a better grasp of the political science domain, as well as broaden the possibility of future visualization approaches from a multidisciplinary perspective.</p><h4 id="robohop-segment-based-topological-map-representation-for-open-world-visual-navigation" tabindex="-1"><a class="header-anchor" href="#robohop-segment-based-topological-map-representation-for-open-world-visual-navigation"><span>RoboHop: Segment-based Topological Map Representation for Open-World Visual Navigation</span></a></h4><p><strong>Authors</strong>: Sourav Garg, Krishan Rana, Mehdi Hosseinzadeh, Lachlan Mares, Niko Sünderhauf, Feras Dayoub, Ian Reid</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.05792v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.05792v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Mapping is crucial for spatial reasoning, planning and robot navigation. Existing approaches range from metric, which require precise geometry-based optimization, to purely topological, where image-as-node based graphs lack explicit object-level reasoning and interconnectivity. In this paper, we propose a novel topological representation of an environment based on &quot;image segments&quot;, which are semantically meaningful and open-vocabulary queryable, conferring several advantages over previous works based on pixel-level features. Unlike 3D scene graphs, we create a purely topological graph with segments as nodes, where edges are formed by a) associating segment-level descriptors between pairs of consecutive images and b) connecting neighboring segments within an image using their pixel centroids. This unveils a &quot;continuous sense of a place&quot;, defined by inter-image persistence of segments along with their intra-image neighbours. It further enables us to represent and update segment-level descriptors through neighborhood aggregation using graph convolution layers, which improves robot localization based on segment-level retrieval. Using real-world data, we show how our proposed map representation can be used to i) generate navigation plans in the form of &quot;hops over segments&quot; and ii) search for target objects using natural language queries describing spatial relations of objects. Furthermore, we quantitatively analyze data association at the segment level, which underpins inter-image connectivity during mapping and segment-level localization when revisiting the same place. Finally, we show preliminary trials on segment-level `hopping&#39; based zero-shot real-world navigation. Project page with supplementary details: oravus.github.io/RoboHop/</p><h4 id="exploring-the-potential-of-human-llm-synergy-in-advancing-qualitative-analysis-a-case-study-on-mental-illness-stigma" tabindex="-1"><a class="header-anchor" href="#exploring-the-potential-of-human-llm-synergy-in-advancing-qualitative-analysis-a-case-study-on-mental-illness-stigma"><span>Exploring the Potential of Human-LLM Synergy in Advancing Qualitative Analysis: A Case Study on Mental-Illness Stigma</span></a></h4><p><strong>Authors</strong>: Han Meng, Yitian Yang, Yunan Li, Jungup Lee, Yi-Chieh Lee</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.05758v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.05758v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Qualitative analysis is a challenging, yet crucial aspect of advancing research in the field of Human-Computer Interaction (HCI). Recent studies show that large language models (LLMs) can perform qualitative coding within existing schemes, but their potential for collaborative human-LLM discovery and new insight generation in qualitative analysis is still underexplored. To bridge this gap and advance qualitative analysis by harnessing the power of LLMs, we propose CHALET, a novel methodology that leverages the human-LLM collaboration paradigm to facilitate conceptualization and empower qualitative research. The CHALET approach involves LLM-supported data collection, performing both human and LLM deductive coding to identify disagreements, and performing collaborative inductive coding on these disagreement cases to derive new conceptual insights. We validated the effectiveness of CHALET through its application to the attribution model of mental-illness stigma, uncovering implicit stigmatization themes on cognitive, emotional and behavioral dimensions. We discuss the implications for future research, methodology, and the transdisciplinary opportunities CHALET presents for the HCI community and beyond.</p><h4 id="beyond-prompts-learning-from-human-communication-for-enhanced-ai-intent-alignment" tabindex="-1"><a class="header-anchor" href="#beyond-prompts-learning-from-human-communication-for-enhanced-ai-intent-alignment"><span>Beyond Prompts: Learning from Human Communication for Enhanced AI Intent Alignment</span></a></h4><p><strong>Authors</strong>: Yoonsu Kim, Kihoon Son, Seoyoung Kim, Juho Kim</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.05678v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.05678v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: AI intent alignment, ensuring that AI produces outcomes as intended by users, is a critical challenge in human-AI interaction. The emergence of generative AI, including LLMs, has intensified the significance of this problem, as interactions increasingly involve users specifying desired results for AI systems. In order to support better AI intent alignment, we aim to explore human strategies for intent specification in human-human communication. By studying and comparing human-human and human-LLM communication, we identify key strategies that can be applied to the design of AI systems that are more effective at understanding and aligning with user intent. This study aims to advance toward a human-centered AI system by bringing together human communication strategies for the design of AI systems.</p><h4 id="ai-in-your-toolbox-a-plugin-for-generating-renderings-from-3d-models" tabindex="-1"><a class="header-anchor" href="#ai-in-your-toolbox-a-plugin-for-generating-renderings-from-3d-models"><span>AI in Your Toolbox: A Plugin for Generating Renderings from 3D Models</span></a></h4><p><strong>Authors</strong>: Mingming Wang</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.05627v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.05627v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: With the rapid development of LLMs and AIGC technology, we present a Rhino platform plugin utilizing stable diffusion technology. This plugin enables real-time application deployment from 3D modeling software, integrating stable diffusion models with Rhino&#39;s features. It offers intelligent design functions, real-time feedback, and cross-platform linkage, enhancing design efficiency and quality. Our ongoing efforts focus on optimizing the plugin to further advance AI applications in CAD, empowering designers with smarter and more efficient design tools. Our goal is to provide designers with enhanced capabilities for creating exceptional designs in an increasingly AI-driven CAD environment.</p><h4 id="one-vs-many-comprehending-accurate-information-from-multiple-erroneous-and-inconsistent-ai-generations" tabindex="-1"><a class="header-anchor" href="#one-vs-many-comprehending-accurate-information-from-multiple-erroneous-and-inconsistent-ai-generations"><span>One vs. Many: Comprehending Accurate Information from Multiple Erroneous and Inconsistent AI Generations</span></a></h4><p><strong>Authors</strong>: Yoonjoo Lee, Kihoon Son, Tae Soo Kim, Jisu Kim, John Joon Young Chung, Eytan Adar, Juho Kim</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.05581v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.05581v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: As Large Language Models (LLMs) are nondeterministic, the same input can generate different outputs, some of which may be incorrect or hallucinated. If run again, the LLM may correct itself and produce the correct answer. Unfortunately, most LLM-powered systems resort to single results which, correct or not, users accept. Having the LLM produce multiple outputs may help identify disagreements or alternatives. However, it is not obvious how the user will interpret conflicts or inconsistencies. To this end, we investigate how users perceive the AI model and comprehend the generated information when they receive multiple, potentially inconsistent, outputs. Through a preliminary study, we identified five types of output inconsistencies. Based on these categories, we conducted a study (N=252) in which participants were given one or more LLM-generated passages to an information-seeking question. We found that inconsistency within multiple LLM-generated outputs lowered the participants&#39; perceived AI capacity, while also increasing their comprehension of the given information. Specifically, we observed that this positive effect of inconsistencies was most significant for participants who read two passages, compared to those who read three. Based on these findings, we present design implications that, instead of regarding LLM output inconsistencies as a drawback, we can reveal the potential inconsistencies to transparently indicate the limitations of these models and promote critical LLM usage.</p><h4 id="intelligent-ec-rearview-mirror-enhancing-driver-safety-with-dynamic-glare-mitigation-via-cloud-edge-collaboration" tabindex="-1"><a class="header-anchor" href="#intelligent-ec-rearview-mirror-enhancing-driver-safety-with-dynamic-glare-mitigation-via-cloud-edge-collaboration"><span>Intelligent EC Rearview Mirror: Enhancing Driver Safety with Dynamic Glare Mitigation via Cloud Edge Collaboration</span></a></h4><p><strong>Authors</strong>: Junyi Yang, Zefei Xu, Huayi Lai, Hongjian Chen, Sifan Kong, Yutong Wu, Huan Yang</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.05579v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.05579v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Sudden glare from trailing vehicles significantly increases driving safety risks. Existing anti-glare technologies such as electronic, manually-adjusted, and electrochromic rearview mirrors, are expensive and lack effective adaptability in different lighting conditions. To address these issues, our research introduces an intelligent rearview mirror system utilizing novel all-liquid electrochromic technology. This system integrates IoT with ensemble and federated learning within a cloud edge collaboration framework, dynamically controlling voltage to effectively eliminate glare and maintain clear visibility. Utilizing an ensemble learning model, it automatically adjusts mirror transmittance based on light intensity, achieving a low RMSE of 0.109 on the test set. Furthermore, the system leverages federated learning for distributed data training across devices, which enhances privacy and updates the cloud model continuously. Distinct from conventional methods, our experiment utilizes the Schmidt-Clausen and Bindels de Boer 9-point scale with TOPSIS for comprehensive evaluation of rearview mirror glare. Designed to be convenient and costeffective, this system demonstrates how IoT and AI can significantly enhance rearview mirror anti-glare performance.</p><h4 id="investigating-interaction-modes-and-user-agency-in-human-llm-collaboration-for-domain-specific-data-analysis" tabindex="-1"><a class="header-anchor" href="#investigating-interaction-modes-and-user-agency-in-human-llm-collaboration-for-domain-specific-data-analysis"><span>Investigating Interaction Modes and User Agency in Human-LLM Collaboration for Domain-Specific Data Analysis</span></a></h4><p><strong>Authors</strong>: Jiajing Guo, Vikram Mohanty, Jorge Piazentin Ono, Hongtao Hao, Liang Gou, Liu Ren</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.05548v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.05548v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Despite demonstrating robust capabilities in performing tasks related to general-domain data-operation tasks, Large Language Models (LLMs) may exhibit shortcomings when applied to domain-specific tasks. We consider the design of domain-specific AI-powered data analysis tools from two dimensions: interaction and user agency. We implemented two design probes that fall on the two ends of the two dimensions: an open-ended high agency (OHA) prototype and a structured low agency (SLA) prototype. We conducted an interview study with nine data scientists to investigate (1) how users perceived the LLM outputs for data analysis assistance, and (2) how the two test design probes, OHA and SLA, affected user behavior, performance, and perceptions. Our study revealed insights regarding participants&#39; interactions with LLMs, how they perceived the results, and their desire for explainability concerning LLM outputs, along with a noted need for collaboration with other users, and how they envisioned the utility of LLMs in their workflow.</p><h4 id="predicting-cognitive-load-using-sensor-data-in-a-literacy-game" tabindex="-1"><a class="header-anchor" href="#predicting-cognitive-load-using-sensor-data-in-a-literacy-game"><span>Predicting Cognitive Load Using Sensor Data in a Literacy Game</span></a></h4><p><strong>Authors</strong>: Minghao Cai, Carrie Demmans Epp</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.05543v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.05543v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Educational games are being increasingly used to support self-paced learning. However, educators and system designers often face challenges in monitoring student affect and cognitive load. Existing assessments in game-based learning environments (GBLEs) tend to focus more on outcomes rather than processes, potentially overlooking key aspects of the learning journey that include learner affect and cognitive load. To address this issue, we collected data and trained a model to track learner cognitive load while they used an online literacy game for English. We collected affect-related physiological data and pupil data during gameplay to enable the development of models that identify these latent characteristics of learner processes. Our model indicates the feasibility of using these data to track cognitive load in GBLEs. Our multimodal model distinguished different levels of cognitive load, achieving the highest Kappa (.417) and accuracy (70%). Our model reveals the importance of including affect-related features (i.e., EDA and heart rate) when predicting cognitive load and extends recent findings suggesting the benefit of using multiple channels when modeling latent aspects of learner processes. Findings also suggest that cognitive load tracking could now be used to facilitate the creation of personalized learning experiences.</p><h4 id="dis-placed-contributions-uncovering-hidden-hurdles-to-collaborative-writing-involving-non-native-speakers-native-speakers-and-ai-powered-editing-tools" tabindex="-1"><a class="header-anchor" href="#dis-placed-contributions-uncovering-hidden-hurdles-to-collaborative-writing-involving-non-native-speakers-native-speakers-and-ai-powered-editing-tools"><span>(Dis)placed Contributions: Uncovering Hidden Hurdles to Collaborative Writing Involving Non-Native Speakers, Native Speakers, and AI-Powered Editing Tools</span></a></h4><p><strong>Authors</strong>: Yimin Xiao, Yuewen Chen, Naomi Yamashita, Yuexi Chen, Zhicheng Liu, Ge Gao</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.05474v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.05474v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Content creation today often takes place via collaborative writing. A longstanding interest of CSCW research lies in understanding and promoting the coordination between co-writers. However, little attention has been paid to individuals who write in their non-native language and to co-writer groups involving them. We present a mixed-method study that fills the above gap. Our participants included 32 co-writer groups, each consisting of one native speaker (NS) of English and one non-native speaker (NNS) with limited proficiency. They performed collaborative writing adopting two different workflows: half of the groups began with NNSs taking the first editing turn and half had NNSs act after NSs. Our data revealed a &quot;late-mover disadvantage&quot; exclusively experienced by NNSs: an NNS&#39;s ideational contributions to the joint document were suppressed when their editing turn was placed after an NS&#39;s turn, as opposed to ahead of it. Surprisingly, editing help provided by AI-powered tools did not exempt NNSs from being disadvantaged. Instead, it triggered NSs&#39; overestimation of NNSs&#39; English proficiency and agency displayed in the writing, introducing unintended tensions into the collaboration. These findings shed light on the fair assessment and effective promotion of a co-writer&#39;s contributions in language diverse settings. In particular, they underscore the necessity of disentangling contributions made to the ideational, expressional, and lexical aspects of the joint writing.</p><h2 id="_2024-05-08" tabindex="-1"><a class="header-anchor" href="#_2024-05-08"><span>2024-05-08</span></a></h2><h4 id="studying-self-care-with-generative-ai-tools-lessons-for-design" tabindex="-1"><a class="header-anchor" href="#studying-self-care-with-generative-ai-tools-lessons-for-design"><span>Studying Self-Care with Generative AI Tools: Lessons for Design</span></a></h4><p><strong>Authors</strong>: Tara Capel, Bernd Ploderer, Filip Bircanin, Simon Hanmer, Jamie Yates, Jiaxuan Wang, Kai Ling Khor, Tuck Wah Leong, Greg Wadley, Michelle Newcomb</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.05458v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.05458v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: The rise of generative AI presents new opportunities for the understanding and practice of self-care through its capability to generate varied content, including self-care suggestions via text and images, and engage in dialogue with users over time. However, there are also concerns about accuracy and trustworthiness of self-care advice provided via AI. This paper reports our findings from workshops, diaries, and interviews with five researchers and 24 participants to explore their experiences and use of generative AI for self-care. We analyze our findings to present a framework for the use of generative AI to support five types of self-care, - advice seeking, mentorship, resource creation, social simulation, and therapeutic self-expression - mapped across two dimensions - expertise and modality. We discuss how these practices shift the role of technologies for self-care from merely offering information to offering personalized advice and supporting creativity for reflection, and we offer suggestions for using the framework to investigate new self-care designs.</p><h4 id="the-power-of-absence-thinking-with-archival-theory-in-algorithmic-design" tabindex="-1"><a class="header-anchor" href="#the-power-of-absence-thinking-with-archival-theory-in-algorithmic-design"><span>The Power of Absence: Thinking with Archival Theory in Algorithmic Design</span></a></h4><p><strong>Authors</strong>: Jihan Sherman, Romi Morrison, Lauren Klein, Daniela K. Rosner</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.05420v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.05420v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: This paper explores the value of archival theory as a means of grappling with bias in algorithmic design. Rather than seek to mitigate biases perpetuated by datasets and algorithmic systems, archival theory offers a reframing of bias itself. Drawing on a range of archival theory from the fields of history, literary and cultural studies, Black studies, and feminist STS, we propose absence-as power, presence, and productive-as a concept that might more securely anchor investigations into the causes of algorithmic bias, and that can prompt more capacious, creative, and joyful future work. This essay, in turn, can intervene into the technical as well as the social, historical, and political structures that serve as sources of bias.</p><h4 id="permalife-of-the-archive-archaeogaming-as-queergaming" tabindex="-1"><a class="header-anchor" href="#permalife-of-the-archive-archaeogaming-as-queergaming"><span>Permalife Of The Archive: Archaeogaming As Queergaming</span></a></h4><p><strong>Authors</strong>: Florence Smith Nicholls</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.05411v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.05411v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Archaeogaming and queer games studies have both grown as paradigms in the last decade. The former broadly refers to the archaeological study of games, while the later concerns the application of queer theory to the medium. To date, there has been limited engagement of archaeogamers with queer games scholarship, and vice versa. This article argues that there are epistomological parallels between the two; as they are both concerned with the limits and ethics of representation, the personal and political contexts of game development and engagement with video games through transgressive play. The paper is structured around an extended literature review and three vignettes that reflect on the author&#39;s personal experience of conducting archaeogaming research, an ethnographic study of Wurm Online, an archaeological survey of Elden Ring and a player study of the generative archaeology game Nothing Beside Remains. While archaeogaming can learn from the centring of subjective lived experience and labour in the queer games sphere, archaeogaming as a form of game preservation can also benefit queer games studies.</p><h4 id="they-are-uncultured-unveiling-covert-harms-and-social-threats-in-llm-generated-conversations" tabindex="-1"><a class="header-anchor" href="#they-are-uncultured-unveiling-covert-harms-and-social-threats-in-llm-generated-conversations"><span>&quot;They are uncultured&quot;: Unveiling Covert Harms and Social Threats in LLM Generated Conversations</span></a></h4><p><strong>Authors</strong>: Preetam Prabhu Srikar Dammu, Hayoung Jung, Anjali Singh, Monojit Choudhury, Tanushree Mitra</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.05378v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.05378v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Large language models (LLMs) have emerged as an integral part of modern societies, powering user-facing applications such as personal assistants and enterprise applications like recruitment tools. Despite their utility, research indicates that LLMs perpetuate systemic biases. Yet, prior works on LLM harms predominantly focus on Western concepts like race and gender, often overlooking cultural concepts from other parts of the world. Additionally, these studies typically investigate &quot;harm&quot; as a singular dimension, ignoring the various and subtle forms in which harms manifest. To address this gap, we introduce the Covert Harms and Social Threats (CHAST), a set of seven metrics grounded in social science literature. We utilize evaluation models aligned with human assessments to examine the presence of covert harms in LLM-generated conversations, particularly in the context of recruitment. Our experiments reveal that seven out of the eight LLMs included in this study generated conversations riddled with CHAST, characterized by malign views expressed in seemingly neutral language unlikely to be detected by existing methods. Notably, these LLMs manifested more extreme views and opinions when dealing with non-Western concepts like caste, compared to Western ones such as race.</p><h4 id="quallm-an-llm-based-framework-to-extract-quantitative-insights-from-online-forums" tabindex="-1"><a class="header-anchor" href="#quallm-an-llm-based-framework-to-extract-quantitative-insights-from-online-forums"><span>QuaLLM: An LLM-based Framework to Extract Quantitative Insights from Online Forums</span></a></h4><p><strong>Authors</strong>: Varun Nagaraj Rao, Eesha Agarwal, Samantha Dalal, Dan Calacci, Andrés Monroy-Hernández</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.05345v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.05345v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Online discussion forums provide crucial data to understand the concerns of a wide range of real-world communities. However, the typical qualitative and quantitative methods used to analyze those data, such as thematic analysis and topic modeling, are infeasible to scale or require significant human effort to translate outputs to human readable forms. This study introduces QuaLLM, a novel LLM-based framework to analyze and extract quantitative insights from text data on online forums. The framework consists of a novel prompting methodology and evaluation strategy. We applied this framework to analyze over one million comments from two Reddit&#39;s rideshare worker communities, marking the largest study of its type. We uncover significant worker concerns regarding AI and algorithmic platform decisions, responding to regulatory calls about worker insights. In short, our work sets a new precedent for AI-assisted quantitative data analysis to surface concerns from online forums.</p><h4 id="community-guidelines-make-this-the-best-party-on-the-internet-an-in-depth-study-of-online-platforms-content-moderation-policies" tabindex="-1"><a class="header-anchor" href="#community-guidelines-make-this-the-best-party-on-the-internet-an-in-depth-study-of-online-platforms-content-moderation-policies"><span>&quot;Community Guidelines Make this the Best Party on the Internet&quot;: An In-Depth Study of Online Platforms&#39; Content Moderation Policies</span></a></h4><p><strong>Authors</strong>: Brennan Schaffner, Arjun Nitin Bhagoji, Siyuan Cheng, Jacqueline Mei, Jay L. Shen, Grace Wang, Marshini Chetty, Nick Feamster, Genevieve Lakier, Chenhao Tan</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.05225v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.05225v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Moderating user-generated content on online platforms is crucial for balancing user safety and freedom of speech. Particularly in the United States, platforms are not subject to legal constraints prescribing permissible content. Each platform has thus developed bespoke content moderation policies, but there is little work towards a comparative understanding of these policies across platforms and topics. This paper presents the first systematic study of these policies from the 43 largest online platforms hosting user-generated content, focusing on policies around copyright infringement, harmful speech, and misleading content. We build a custom web-scraper to obtain policy text and develop a unified annotation scheme to analyze the text for the presence of critical components. We find significant structural and compositional variation in policies across topics and platforms, with some variation attributable to disparate legal groundings. We lay the groundwork for future studies of ever-evolving content moderation policies and their impact on users.</p><h4 id="the-potential-and-implications-of-generative-ai-on-hci-education" tabindex="-1"><a class="header-anchor" href="#the-potential-and-implications-of-generative-ai-on-hci-education"><span>The Potential and Implications of Generative AI on HCI Education</span></a></h4><p><strong>Authors</strong>: Ahmed Kharrufa, Ian G Johnson</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.05154v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.05154v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Generative AI (GAI) is impacting teaching and learning directly or indirectly across a range of subjects and disciplines. As educators, we need to understand the potential and limitations of AI in HCI education and ensure our graduating HCI students are aware of the potential and limitations of AI in HCI. In this paper, we report on the main pedagogical insights gained from the inclusion of generative AI into a 10 week undergraduate module. We designed the module to encourage student experimentation with GAI models as part of the design brief requirement and planned practical sessions and discussions. Our insights are based on replies to a survey sent out to the students after completing the module. Our key findings, for HCI educators, report on the use of AI as a persona for developing project ideas and creating resources for design, and AI as a mirror for reflecting students&#39; understanding of key concepts and ideas and highlighting knowledge gaps. We also discuss potential pitfalls that should be considered and the need to assess students&#39; literacies and assumptions of GAIs as pedagogical tools. Finally, we put forward the case for educators to take the opportunities GAI presents as an educational tool and be experimental, creative, and courageous in their practice. We end with a discussion of our findings in relation to the TPACK framework in HCI.</p><h4 id="concerns-on-bias-in-large-language-models-when-creating-synthetic-personae" tabindex="-1"><a class="header-anchor" href="#concerns-on-bias-in-large-language-models-when-creating-synthetic-personae"><span>Concerns on Bias in Large Language Models when Creating Synthetic Personae</span></a></h4><p><strong>Authors</strong>: Helena A. Haxvig</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.05080v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.05080v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: This position paper explores the benefits, drawbacks, and ethical considerations of incorporating synthetic personae in HCI research, particularly focusing on the customization challenges beyond the limitations of current Large Language Models (LLMs). These perspectives are derived from the initial results of a sub-study employing vignettes to showcase the existence of bias within black-box LLMs and explore methods for manipulating them. The study aims to establish a foundation for understanding the challenges associated with these models, emphasizing the necessity of thorough testing before utilizing them to create synthetic personae for HCI research.</p><h4 id="challenges-for-responsible-ai-design-and-workflow-integration-in-healthcare-a-case-study-of-automatic-feeding-tube-qualification-in-radiology" tabindex="-1"><a class="header-anchor" href="#challenges-for-responsible-ai-design-and-workflow-integration-in-healthcare-a-case-study-of-automatic-feeding-tube-qualification-in-radiology"><span>Challenges for Responsible AI Design and Workflow Integration in Healthcare: A Case Study of Automatic Feeding Tube Qualification in Radiology</span></a></h4><p><strong>Authors</strong>: Anja Thieme, Abhijith Rajamohan, Benjamin Cooper, Heather Groombridge, Robert Simister, Barney Wong, Nicholas Woznitza, Mark Ames Pinnock, Maria Teodora Wetscherek, Cecily Morrison, Hannah Richardson, Fernando Pérez-García, Stephanie L. Hyland, Shruthi Bannur, Daniel C. Castro, Kenza Bouzid, Anton Schwaighofer, Mercy Ranjit, Harshita Sharma, Matthew P. Lungren, Ozan Oktay, Javier Alvarez-Valle, Aditya Nori, Stephen Harris, Joseph Jacob</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.05299v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.05299v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Nasogastric tubes (NGTs) are feeding tubes that are inserted through the nose into the stomach to deliver nutrition or medication. If not placed correctly, they can cause serious harm, even death to patients. Recent AI developments demonstrate the feasibility of robustly detecting NGT placement from Chest X-ray images to reduce risks of sub-optimally or critically placed NGTs being missed or delayed in their detection, but gaps remain in clinical practice integration. In this study, we present a human-centered approach to the problem and describe insights derived following contextual inquiry and in-depth interviews with 15 clinical stakeholders. The interviews helped understand challenges in existing workflows, and how best to align technical capabilities with user needs and expectations. We discovered the trade-offs and complexities that need consideration when choosing suitable workflow stages, target users, and design configurations for different AI proposals. We explored how to balance AI benefits and risks for healthcare staff and patients within broader organizational and medical-legal constraints. We also identified data issues related to edge cases and data biases that affect model training and evaluation; how data documentation practices influence data preparation and labelling; and how to measure relevant AI outcomes reliably in future evaluations. We discuss how our work informs design and development of AI applications that are clinically useful, ethical, and acceptable in real-world healthcare services.</p><h4 id="impact-of-tone-aware-explanations-in-recommender-systems" tabindex="-1"><a class="header-anchor" href="#impact-of-tone-aware-explanations-in-recommender-systems"><span>Impact of Tone-Aware Explanations in Recommender Systems</span></a></h4><p><strong>Authors</strong>: Ayano Okoso, Keisuke Otaki, Satoshi Koide, Yukino Baba</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.05061v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.05061v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: In recommender systems, the presentation of explanations plays a crucial role in supporting users&#39; decision-making processes. Although numerous existing studies have focused on the effects (transparency or persuasiveness) of explanation content, explanation expression is largely overlooked. Tone, such as formal and humorous, is directly linked to expressiveness and is an important element in human communication. However, studies on the impact of tone on explanations within the context of recommender systems are insufficient. Therefore, this study investigates the effect of explanation tones through an online user study from three aspects: perceived effects, domain differences, and user attributes. We create a dataset using a large language model to generate fictional items and explanations with various tones in the domain of movies, hotels, and home products. Collected data analysis reveals different perceived effects of tones depending on the domains. Moreover, user attributes such as age and personality traits are found to influence the impact of tone. This research underscores the critical role of tones in explanations within recommender systems, suggesting that attention to tone can enhance user experience.</p><h4 id="overcoming-anchoring-bias-the-potential-of-ai-and-xai-based-decision-support" tabindex="-1"><a class="header-anchor" href="#overcoming-anchoring-bias-the-potential-of-ai-and-xai-based-decision-support"><span>Overcoming Anchoring Bias: The Potential of AI and XAI-based Decision Support</span></a></h4><p><strong>Authors</strong>: Felix Haag, Carlo Stingl, Katrin Zerfass, Konstantin Hopf, Thorsten Staake</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.04972v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.04972v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Information systems (IS) are frequently designed to leverage the negative effect of anchoring bias to influence individuals&#39; decision-making (e.g., by manipulating purchase decisions). Recent advances in Artificial Intelligence (AI) and the explanations of its decisions through explainable AI (XAI) have opened new opportunities for mitigating biased decisions. So far, the potential of these technological advances to overcome anchoring bias remains widely unclear. To this end, we conducted two online experiments with a total of N=390 participants in the context of purchase decisions to examine the impact of AI and XAI-based decision support on anchoring bias. Our results show that AI alone and its combination with XAI help to mitigate the negative effect of anchoring bias. Ultimately, our findings have implications for the design of AI and XAI-based decision support and IS to overcome cognitive biases.</p><h4 id="harmonizing-program-induction-with-rate-distortion-theory" tabindex="-1"><a class="header-anchor" href="#harmonizing-program-induction-with-rate-distortion-theory"><span>Harmonizing Program Induction with Rate-Distortion Theory</span></a></h4><p><strong>Authors</strong>: Hanqi Zhou, David G. Nagy, Charley M. Wu</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.05294v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.05294v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Many aspects of human learning have been proposed as a process of constructing mental programs: from acquiring symbolic number representations to intuitive theories about the world. In parallel, there is a long-tradition of using information processing to model human cognition through Rate Distortion Theory (RDT). Yet, it is still poorly understood how to apply RDT when mental representations take the form of programs. In this work, we adapt RDT by proposing a three way trade-off among rate (description length), distortion (error), and computational costs (search budget). We use simulations on a melody task to study the implications of this trade-off, and show that constructing a shared program library across tasks provides global benefits. However, this comes at the cost of sensitivity to curricula, which is also characteristic of human learners. Finally, we use methods from partial information decomposition to generate training curricula that induce more effective libraries and better generalization.</p><h4 id="smart-portable-computer" tabindex="-1"><a class="header-anchor" href="#smart-portable-computer"><span>Smart Portable Computer</span></a></h4><p><strong>Authors</strong>: Niladri Das</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.05292v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.05292v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Amidst the COVID-19 pandemic, with many organizations, schools, colleges, and universities transitioning to virtual platforms, students encountered difficulties in acquiring PCs such as desktops or laptops. The starting prices, around 15,000 INR, often failed to offer adequate system specifications, posing a challenge for consumers. Additionally, those reliant on laptops for work found the conventional approach cumbersome. Enter the &quot;Portable Smart Computer,&quot; a leap into the future of computing. This innovative device boasts speed and performance comparable to traditional desktops but in a compact, energy-efficient, and cost-effective package. It delivers a seamless desktop experience, whether one is editing documents, browsing multiple tabs, managing spreadsheets, or creating presentations. Moreover, it supports programming languages like Python, C, C++, as well as compilers such as Keil and Xilinx, catering to the needs of programmers.</p><h4 id="practice-informed-patterns-for-organising-large-groups-in-distributed-mixed-reality-collaboration" tabindex="-1"><a class="header-anchor" href="#practice-informed-patterns-for-organising-large-groups-in-distributed-mixed-reality-collaboration"><span>Practice-informed Patterns for Organising Large Groups in Distributed Mixed Reality Collaboration</span></a></h4><p><strong>Authors</strong>: Emily Wong, Juan Sánchez Esquivel, Jens Emil Grønbæk, Germán Leiva, Eduardo Velloso</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.04873v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.04873v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Collaborating across dissimilar, distributed spaces presents numerous challenges for computer-aided spatial communication. Mixed reality (MR) can blend selected surfaces, allowing collaborators to work in blended f-formations (facing formations), even when their workstations are physically misaligned. Since collaboration often involves more than just participant pairs, this research examines how we might scale MR experiences for large-group collaboration. To do so, this study recruited collaboration designers (CDs) to evaluate and reimagine MR for large-scale collaboration. These CDs were engaged in a four-part user study that involved a technology probe, a semi-structured interview, a speculative low-fidelity prototyping activity and a validation session. The outcomes of this paper contribute (1) a set of collaboration design principles to inspire future computer-supported collaborative work, (2) eight collaboration patterns for blended f-formations and collaboration at scale and (3) theoretical implications for f-formations and space-place relationships. As a result, this work creates a blueprint for scaling collaboration across distributed spaces.</p><h4 id="the-impact-of-perceived-tone-age-and-gender-on-voice-assistant-persuasiveness-in-the-context-of-product-recommendations" tabindex="-1"><a class="header-anchor" href="#the-impact-of-perceived-tone-age-and-gender-on-voice-assistant-persuasiveness-in-the-context-of-product-recommendations"><span>The Impact of Perceived Tone, Age, and Gender on Voice Assistant Persuasiveness in the Context of Product Recommendations</span></a></h4><p><strong>Authors</strong>: Sabid Bin Habib Pias, Ran Huang, Donald Williamson, Minjeong Kim, Apu Kapadia</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.04791v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.04791v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Voice Assistants (VAs) can assist users in various everyday tasks, but many users are reluctant to rely on VAs for intricate tasks like online shopping. This study aims to examine whether the vocal characteristics of VAs can serve as an effective tool to persuade users and increase user engagement with VAs in online shopping. Prior studies have demonstrated that the perceived tone, age, and gender of a voice influence the perceived persuasiveness of the speaker in interpersonal interactions. Furthermore, persuasion in product communication has been shown to affect purchase decisions in online shopping. We investigate whether variations in a VA voice&#39;s perceived tone, age, and gender characteristics can persuade users, and ultimately affect their purchase decisions. Our experimental study showed that participants were more persuaded to make purchase decisions by VA voices having positive or neutral tones as well as middle-aged male or younger female voices. Our results suggest that VA designers should offer users the ability to easily customize VA voices with a range of tones, ages, and genders. This customization can enhance user comfort and enjoyment, potentially leading to higher engagement with VAs. Additionally, we discuss the boundaries of ethical persuasion, emphasizing the importance of safeguarding users&#39; interests against unwarranted manipulation.</p><h2 id="_2024-05-07" tabindex="-1"><a class="header-anchor" href="#_2024-05-07"><span>2024-05-07</span></a></h2><h4 id="metaverse-survey-tutorial-exploring-key-requirements-technologies-standards-applications-challenges-and-perspectives" tabindex="-1"><a class="header-anchor" href="#metaverse-survey-tutorial-exploring-key-requirements-technologies-standards-applications-challenges-and-perspectives"><span>Metaverse Survey &amp; Tutorial: Exploring Key Requirements, Technologies, Standards, Applications, Challenges, and Perspectives</span></a></h4><p><strong>Authors</strong>: Danda B. Rawat, Hassan El alami, Desta Haileselassie Hagos</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.04718v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.04718v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: In this paper, we present a comprehensive survey of the metaverse, envisioned as a transformative dimension of next-generation Internet technologies. This study not only outlines the structural components of our survey but also makes a substantial scientific contribution by elucidating the foundational concepts underlying the emergence of the metaverse. We analyze its architecture by defining key characteristics and requirements, thereby illuminating the nascent reality set to revolutionize digital interactions. Our analysis emphasizes the importance of collaborative efforts in developing metaverse standards, thereby fostering a unified understanding among industry stakeholders, organizations, and regulatory bodies. We extend our scrutiny to critical technologies integral to the metaverse, including interactive experiences, communication technologies, ubiquitous computing, digital twins, artificial intelligence, and cybersecurity measures. For each technological domain, we rigorously assess current contributions, principal techniques, and representative use cases, providing a nuanced perspective on their potential impacts. Furthermore, we delve into the metaverse&#39;s diverse applications across education, healthcare, business, social interactions, industrial sectors, defense, and mission-critical operations, highlighting its extensive utility. Each application is thoroughly analyzed, demonstrating its value and addressing associated challenges. The survey concludes with an overview of persistent challenges and future directions, offering insights into essential considerations and strategies necessary to harness the full potential of the metaverse. Through this detailed investigation, our goal is to articulate the scientific contributions of this survey paper, transcending a mere structural overview to highlight the transformative implications of the metaverse.</p><h4 id="generative-ai-as-a-metacognitive-agent-a-comparative-mixed-method-study-with-human-participants-on-icf-mimicking-exam-performance" tabindex="-1"><a class="header-anchor" href="#generative-ai-as-a-metacognitive-agent-a-comparative-mixed-method-study-with-human-participants-on-icf-mimicking-exam-performance"><span>Generative AI as a metacognitive agent: A comparative mixed-method study with human participants on ICF-mimicking exam performance</span></a></h4><p><strong>Authors</strong>: Jelena Pavlovic, Jugoslav Krstic, Luka Mitrovic, Djordje Babic, Adrijana Milosavljevic, Milena Nikolic, Tijana Karaklic, Tijana Mitrovic</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.05285v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.05285v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: This study investigates the metacognitive capabilities of Large Language Models relative to human metacognition in the context of the International Coaching Federation ICF mimicking exam, a situational judgment test related to coaching competencies. Using a mixed method approach, we assessed the metacognitive performance, including sensitivity, accuracy in probabilistic predictions, and bias, of human participants and five advanced LLMs (GPT-4, Claude-3-Opus 3, Mistral Large, Llama 3, and Gemini 1.5 Pro). The results indicate that LLMs outperformed humans across all metacognitive metrics, particularly in terms of reduced overconfidence, compared to humans. However, both LLMs and humans showed less adaptability in ambiguous scenarios, adhering closely to predefined decision frameworks. The study suggests that Generative AI can effectively engage in human-like metacognitive processing without conscious awareness. Implications of the study are discussed in relation to development of AI simulators that scaffold cognitive and metacognitive aspects of mastering coaching competencies. More broadly, implications of these results are discussed in relation to development of metacognitive modules that lead towards more autonomous and intuitive AI systems.</p><h4 id="towards-human-ai-mutual-learning-a-new-research-paradigm" tabindex="-1"><a class="header-anchor" href="#towards-human-ai-mutual-learning-a-new-research-paradigm"><span>Towards Human-AI Mutual Learning: A New Research Paradigm</span></a></h4><p><strong>Authors</strong>: Xiaomei Wang, Xiaoyu Chen</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.04687v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.04687v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: This paper describes a new research paradigm for studying human-AI collaboration, named &quot;human-AI mutual learning&quot;, defined as the process where humans and AI agents preserve, exchange, and improve knowledge during human-AI collaboration. We describe relevant methodologies, motivations, domain examples, benefits, challenges, and future research agenda under this paradigm.</p><h4 id="responding-to-generative-ai-technologies-with-research-through-design-the-ryelands-ai-lab-as-an-exploratory-study" tabindex="-1"><a class="header-anchor" href="#responding-to-generative-ai-technologies-with-research-through-design-the-ryelands-ai-lab-as-an-exploratory-study"><span>Responding to Generative AI Technologies with Research-through-Design: The Ryelands AI Lab as an Exploratory Study</span></a></h4><p><strong>Authors</strong>: Jesse Josua Benjamin, Joseph Lindley, Elizabeth Edwards, Elisa Rubegni, Tim Korjakow, David Grist, Rhiannon Sharkey</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.04677v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.04677v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Generative AI technologies demand new practical and critical competencies, which call on design to respond to and foster these. We present an exploratory study guided by Research-through-Design, in which we partnered with a primary school to develop a constructionist curriculum centered on students interacting with a generative AI technology. We provide a detailed account of the design of and outputs from the curriculum and learning materials, finding centrally that the reflexive and prolonged <code>hands-on&#39; approach led to a co-development of students&#39; practical and critical competencies. From the study, we contribute guidance for designing constructionist approaches to generative AI technology education; further arguing to do so with </code>critical responsivity.&#39; We then discuss how HCI researchers may leverage constructionist strategies in designing interactions with generative AI technologies; and suggest that Research-through-Design can play an important role as a `rapid response methodology&#39; capable of reacting to fast-evolving, disruptive technologies such as generative AI.</p><h4 id="a-study-on-cognitive-effects-of-canvas-size-for-augmenting-drawing-skill" tabindex="-1"><a class="header-anchor" href="#a-study-on-cognitive-effects-of-canvas-size-for-augmenting-drawing-skill"><span>A Study on Cognitive Effects of Canvas Size for Augmenting Drawing Skill</span></a></h4><p><strong>Authors</strong>: Jize Wang, Kazuhisa Nakano, Daiyannan Chen, Zhengyu Huang, Tsukasa Fukusato, Kazunori Miyata, Haoran Xie</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.05284v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.05284v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: In recent years, the field of generative artificial intelligence, particularly in the domain of image generation, has exerted a profound influence on society. Despite the capability of AI to produce images of high quality, the augmentation of users&#39; drawing abilities through the provision of drawing support systems emerges as a challenging issue. In this study, we propose that a cognitive factor, specifically, the size of the canvas, may exert a considerable influence on the outcomes of imitative drawing sketches when utilizing reference images. To investigate this hypothesis, a web based drawing interface was utilized, designed specifically to evaluate the effect of the canvas size&#39;s proportionality to the reference image on the fidelity of the drawings produced. The findings from our research lend credence to the hypothesis that a drawing interface, featuring a canvas whose dimensions closely match those of the reference image, markedly improves the precision of user-generated sketches.</p><h4 id="corporate-communication-companion-ccc-an-llm-empowered-writing-assistant-for-workplace-social-media" tabindex="-1"><a class="header-anchor" href="#corporate-communication-companion-ccc-an-llm-empowered-writing-assistant-for-workplace-social-media"><span>Corporate Communication Companion (CCC): An LLM-empowered Writing Assistant for Workplace Social Media</span></a></h4><p><strong>Authors</strong>: Zhuoran Lu, Sheshera Mysore, Tara Safavi, Jennifer Neville, Longqi Yang, Mengting Wan</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.04656v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.04656v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Workplace social media platforms enable employees to cultivate their professional image and connect with colleagues in a semi-formal environment. While semi-formal corporate communication poses a unique set of challenges, large language models (LLMs) have shown great promise in helping users draft and edit their social media posts. However, LLMs may fail to capture individualized tones and voices in such workplace use cases, as they often generate text using a &quot;one-size-fits-all&quot; approach that can be perceived as generic and bland. In this paper, we present Corporate Communication Companion (CCC), an LLM-empowered interactive system that helps people compose customized and individualized workplace social media posts. Using need-finding interviews to motivate our system design, CCC decomposes the writing process into two core functions, outline and edit: First, it suggests post outlines based on users&#39; job status and previous posts, and next provides edits with attributions that users can contextually customize. We conducted a within-subjects user study asking participants both to write posts and evaluate posts written by others. The results show that CCC enhances users&#39; writing experience, and audience members rate CCC-enhanced posts as higher quality than posts written using a non-customized writing assistant. We conclude by discussing the implications of LLM-empowered corporate communication.</p><h4 id="affirmativeai-towards-lgbtq-friendly-audit-frameworks-for-large-language-models" tabindex="-1"><a class="header-anchor" href="#affirmativeai-towards-lgbtq-friendly-audit-frameworks-for-large-language-models"><span>AffirmativeAI: Towards LGBTQ+ Friendly Audit Frameworks for Large Language Models</span></a></h4><p><strong>Authors</strong>: Yinru Long, Zilin Ma, Yiyang Mei, Zhaoyuan Su</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.04652v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.04652v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: LGBTQ+ community face disproportionate mental health challenges, including higher rates of depression, anxiety, and suicidal ideation. Research has shown that LGBTQ+ people have been using large language model-based chatbots, such as ChatGPT, for their mental health needs. Despite the potential for immediate support and anonymity these chatbots offer, concerns regarding their capacity to provide empathetic, accurate, and affirming responses remain. In response to these challenges, we propose a framework for evaluating the affirmativeness of LLMs based on principles of affirmative therapy, emphasizing the need for attitudes, knowledge, and actions that support and validate LGBTQ+ experiences. We propose a combination of qualitative and quantitative analyses, hoping to establish benchmarks for &quot;Affirmative AI,&quot; ensuring that LLM-based chatbots can provide safe, supportive, and effective mental health support to LGBTQ+ individuals. We benchmark LLM affirmativeness not as a mental health solution for LGBTQ+ individuals or to claim it resolves their mental health issues, as we highlight the need to consider complex discrimination in the LGBTQ+ community when designing technological aids. Our goal is to evaluate LLMs for LGBTQ+ mental health support since many in the community already use them, aiming to identify potential harms of using general-purpose LLMs in this context.</p><h4 id="enhancing-llm-based-feedback-insights-from-intelligent-tutoring-systems-and-the-learning-sciences" tabindex="-1"><a class="header-anchor" href="#enhancing-llm-based-feedback-insights-from-intelligent-tutoring-systems-and-the-learning-sciences"><span>Enhancing LLM-Based Feedback: Insights from Intelligent Tutoring Systems and the Learning Sciences</span></a></h4><p><strong>Authors</strong>: John Stamper, Ruiwei Xiao, Xinynig Hou</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.04645v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.04645v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: The field of Artificial Intelligence in Education (AIED) focuses on the intersection of technology, education, and psychology, placing a strong emphasis on supporting learners&#39; needs with compassion and understanding. The growing prominence of Large Language Models (LLMs) has led to the development of scalable solutions within educational settings, including generating different types of feedback in Intelligent Tutoring Systems. However, the approach to utilizing these models often involves directly formulating prompts to solicit specific information, lacking a solid theoretical foundation for prompt construction and empirical assessments of their impact on learning. This work advocates careful and caring AIED research by going through previous research on feedback generation in ITS, with emphasis on the theoretical frameworks they utilized and the efficacy of the corresponding design in empirical evaluations, and then suggesting opportunities to apply these evidence-based principles to the design, experiment, and evaluation phases of LLM-based feedback generation. The main contributions of this paper include: an avocation of applying more cautious, theoretically grounded methods in feedback generation in the era of generative AI; and practical suggestions on theory and evidence-based feedback design for LLM-powered ITS.</p><h4 id="unveiling-disparities-in-web-task-handling-between-human-and-web-agent" tabindex="-1"><a class="header-anchor" href="#unveiling-disparities-in-web-task-handling-between-human-and-web-agent"><span>Unveiling Disparities in Web Task Handling Between Human and Web Agent</span></a></h4><p><strong>Authors</strong>: Kihoon Son, Jinhyeon Kwon, DaEun Choi, Tae Soo Kim, Young-Ho Kim, Sangdoo Yun, Juho Kim</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.04497v2" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.04497v2<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: With the advancement of Large-Language Models (LLMs) and Large Vision-Language Models (LVMs), agents have shown significant capabilities in various tasks, such as data analysis, gaming, or code generation. Recently, there has been a surge in research on web agents, capable of performing tasks within the web environment. However, the web poses unforeseeable scenarios, challenging the generalizability of these agents. This study investigates the disparities between human and web agents&#39; performance in web tasks (e.g., information search) by concentrating on planning, action, and reflection aspects during task execution. We conducted a web task study with a think-aloud protocol, revealing distinct cognitive actions and operations on websites employed by humans. Comparative examination of existing agent structures and human behavior with thought processes highlighted differences in knowledge updating and ambiguity handling when performing the task. Humans demonstrated a propensity for exploring and modifying plans based on additional information and investigating reasons for failure. These findings offer insights into designing planning, reflection, and information discovery modules for web agents and designing the capturing method for implicit human knowledge in a web task.</p><h4 id="towards-geographic-inclusion-in-the-evaluation-of-text-to-image-models" tabindex="-1"><a class="header-anchor" href="#towards-geographic-inclusion-in-the-evaluation-of-text-to-image-models"><span>Towards Geographic Inclusion in the Evaluation of Text-to-Image Models</span></a></h4><p><strong>Authors</strong>: Melissa Hall, Samuel J. Bell, Candace Ross, Adina Williams, Michal Drozdzal, Adriana Romero Soriano</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.04457v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.04457v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Rapid progress in text-to-image generative models coupled with their deployment for visual content creation has magnified the importance of thoroughly evaluating their performance and identifying potential biases. In pursuit of models that generate images that are realistic, diverse, visually appealing, and consistent with the given prompt, researchers and practitioners often turn to automated metrics to facilitate scalable and cost-effective performance profiling. However, commonly-used metrics often fail to account for the full diversity of human preference; often even in-depth human evaluations face challenges with subjectivity, especially as interpretations of evaluation criteria vary across regions and cultures. In this work, we conduct a large, cross-cultural study to study how much annotators in Africa, Europe, and Southeast Asia vary in their perception of geographic representation, visual appeal, and consistency in real and generated images from state-of-the art public APIs. We collect over 65,000 image annotations and 20 survey responses. We contrast human annotations with common automated metrics, finding that human preferences vary notably across geographic location and that current metrics do not fully account for this diversity. For example, annotators in different locations often disagree on whether exaggerated, stereotypical depictions of a region are considered geographically representative. In addition, the utility of automatic evaluations is dependent on assumptions about their set-up, such as the alignment of feature extractors with human perception of object similarity or the definition of &quot;appeal&quot; captured in reference datasets used to ground evaluations. We recommend steps for improved automatic and human evaluations.</p><h4 id="large-language-models-cannot-explain-themselves" tabindex="-1"><a class="header-anchor" href="#large-language-models-cannot-explain-themselves"><span>Large Language Models Cannot Explain Themselves</span></a></h4><p><strong>Authors</strong>: Advait Sarkar</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.04382v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.04382v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Large language models can be prompted to produce text. They can also be prompted to produce &quot;explanations&quot; of their output. But these are not really explanations, because they do not accurately reflect the mechanical process underlying the prediction. The illusion that they reflect the reasoning process can result in significant harms. These &quot;explanations&quot; can be valuable, but for promoting critical thinking rather than for understanding the model. I propose a recontextualisation of these &quot;explanations&quot;, using the term &quot;exoplanations&quot; to draw attention to their exogenous nature. I discuss some implications for design and technology, such as the inclusion of appropriate guardrails and responses when models are prompted to generate explanations.</p><h4 id="a-general-model-for-detecting-learner-engagement-implementation-and-evaluation" tabindex="-1"><a class="header-anchor" href="#a-general-model-for-detecting-learner-engagement-implementation-and-evaluation"><span>A General Model for Detecting Learner Engagement: Implementation and Evaluation</span></a></h4><p><strong>Authors</strong>: Somayeh Malekshahi, Javad M. Kheyridoost, Omid Fatemi</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.04251v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.04251v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Considering learner engagement has a mutual benefit for both learners and instructors. Instructors can help learners increase their attention, involvement, motivation, and interest. On the other hand, instructors can improve their instructional performance by evaluating the cumulative results of all learners and upgrading their training programs. This paper proposes a general, lightweight model for selecting and processing features to detect learners&#39; engagement levels while preserving the sequential temporal relationship over time. During training and testing, we analyzed the videos from the publicly available DAiSEE dataset to capture the dynamic essence of learner engagement. We have also proposed an adaptation policy to find new labels that utilize the affective states of this dataset related to education, thereby improving the models&#39; judgment. The suggested model achieves an accuracy of 68.57% in a specific implementation and outperforms the studied state-of-the-art models detecting learners&#39; engagement levels.</p><h4 id="what-impacts-the-quality-of-the-user-answers-when-asked-about-the-current-context" tabindex="-1"><a class="header-anchor" href="#what-impacts-the-quality-of-the-user-answers-when-asked-about-the-current-context"><span>What Impacts the Quality of the User Answers when Asked about the Current Context?</span></a></h4><p><strong>Authors</strong>: Ivano Bison, Haonan Zhao, Fausto Giunchiglia</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.04054v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.04054v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Sensor data provide an objective view of reality but fail to capture the subjective motivations behind an individual&#39;s behavior. This latter information is crucial for learning about the various dimensions of the personal context, thus increasing predictability. The main limitation is the human input, which is often not of the quality that is needed. The work so far has focused on the usually high number of missing answers. The focus of this paper is on \textit{the number of mistakes} made when answering questions. Three are the main contributions of this paper. First, we show that the user&#39;s reaction time, i.e., the time before starting to respond, is the main cause of a low answer quality, where its effects are both direct and indirect, the latter relating to its impact on the completion time, i.e., the time taken to compile the response. Second, we identify the specific exogenous (e.g., the situational or temporal context) and endogenous (e.g., mood, personality traits) factors which have an influence on the reaction time, as well as on the completion time. Third, we show how reaction and completion time compose their effects on the answer quality. The paper concludes with a set of actionable recommendations.</p><h4 id="interaction-design-for-human-ai-choreography-co-creation" tabindex="-1"><a class="header-anchor" href="#interaction-design-for-human-ai-choreography-co-creation"><span>Interaction Design for Human-AI Choreography Co-creation</span></a></h4><p><strong>Authors</strong>: Yimeng Liu</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.03999v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.03999v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Human-AI co-creation aims to combine human and AI strengths for artistic results exceeding individual capabilities. Frameworks exist for painting, music, and poetry, but choreography&#39;s embodied nature demands a dedicated approach. This paper explores AI-assisted choreography techniques (e.g., generative ideation, embodied improvisation) and analyzes interaction design -- how humans and AI collaborate and communicate -- to inform the design considerations of future human-AI choreography co-creation systems.</p><h4 id="sketch-then-generate-providing-incremental-user-feedback-and-guiding-llm-code-generation-through-language-oriented-code-sketches" tabindex="-1"><a class="header-anchor" href="#sketch-then-generate-providing-incremental-user-feedback-and-guiding-llm-code-generation-through-language-oriented-code-sketches"><span>Sketch Then Generate: Providing Incremental User Feedback and Guiding LLM Code Generation through Language-Oriented Code Sketches</span></a></h4><p><strong>Authors</strong>: Chen Zhu-Tian, Zeyu Xiong, Xiaoshuo Yao, Elena Glassman</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.03998v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.03998v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Crafting effective prompts for code generation or editing with Large Language Models (LLMs) is not an easy task. Particularly, the absence of immediate, stable feedback during prompt crafting hinders effective interaction, as users are left to mentally imagine possible outcomes until the code is generated. In response, we introduce Language-Oriented Code Sketching, an interactive approach that provides instant, incremental feedback in the form of code sketches (i.e., incomplete code outlines) during prompt crafting. This approach converts a prompt into a code sketch by leveraging the inherent linguistic structures within the prompt and applying classic natural language processing techniques. The sketch then serves as an intermediate placeholder that not only previews the intended code structure but also guides the LLM towards the desired code, thereby enhancing human-LLM interaction. We conclude by discussing the approach&#39;s applicability and future plans.</p><h4 id="factors-influencing-user-willingness-to-use-sora" tabindex="-1"><a class="header-anchor" href="#factors-influencing-user-willingness-to-use-sora"><span>Factors Influencing User Willingness To Use SORA</span></a></h4><p><strong>Authors</strong>: Gustave Florentin Nkoulou Mvondo, Ben Niu</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.03986v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.03986v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Sora promises to redefine the way visual content is created. Despite its numerous forecasted benefits, the drivers of user willingness to use the text-to-video (T2V) model are unknown. This study extends the extended unified theory of acceptance and use of technology (UTAUT2) with perceived realism and novelty value. Using a purposive sampling method, we collected data from 940 respondents in the US and analyzed the sample using covariance-based structural equation modeling and fuzzy set qualitative comparative analysis (fsQCA). The findings reveal that all hypothesized relationships are supported, with perceived realism emerging as the most influential driver, followed by novelty value. Moreover, fsQCA identifies five configurations leading to high and low willingness to use, and the model demonstrates high predictive validity, contributing to theory advancement. Our study provides valuable insights for developers and marketers, offering guidance for strategic decisions to promote the widespread adoption of T2V models.</p><h4 id="the-fault-in-our-recommendations-on-the-perils-of-optimizing-the-measurable" tabindex="-1"><a class="header-anchor" href="#the-fault-in-our-recommendations-on-the-perils-of-optimizing-the-measurable"><span>The Fault in Our Recommendations: On the Perils of Optimizing the Measurable</span></a></h4><p><strong>Authors</strong>: Omar Besbes, Yash Kanoria, Akshit Kumar</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.03948v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.03948v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Recommendation systems are widespread, and through customized recommendations, promise to match users with options they will like. To that end, data on engagement is collected and used. Most recommendation systems are ranking-based, where they rank and recommend items based on their predicted engagement. However, the engagement signals are often only a crude proxy for utility, as data on the latter is rarely collected or available. This paper explores the following question: By optimizing for measurable proxies, are recommendation systems at risk of significantly under-delivering on utility? If so, how can one improve utility which is seldom measured? To study these questions, we introduce a model of repeated user consumption in which, at each interaction, users select between an outside option and the best option from a recommendation set. Our model accounts for user heterogeneity, with the majority preferring <code>popular&#39;&#39; content, and a minority favoring </code>niche&#39;&#39; content. The system initially lacks knowledge of individual user preferences but can learn them through observations of users&#39; choices over time. Our theoretical and numerical analysis demonstrate that optimizing for engagement can lead to significant utility losses. Instead, we propose a utility-aware policy that initially recommends a mix of popular and niche content. As the platform becomes more forward-looking, our utility-aware policy achieves the best of both worlds: near-optimal utility and near-optimal engagement simultaneously. Our study elucidates an important feature of recommendation systems; given the ability to suggest multiple items, one can perform significant exploration without incurring significant reductions in engagement. By recommending high-risk, high-reward items alongside popular items, systems can enhance discovery of high utility items without significantly affecting engagement.</p><h4 id="collaborative-intelligence-in-sequential-experiments-a-human-in-the-loop-framework-for-drug-discovery" tabindex="-1"><a class="header-anchor" href="#collaborative-intelligence-in-sequential-experiments-a-human-in-the-loop-framework-for-drug-discovery"><span>Collaborative Intelligence in Sequential Experiments: A Human-in-the-Loop Framework for Drug Discovery</span></a></h4><p><strong>Authors</strong>: Jinghai He, Cheng Hua, Yingfei Wang, Zeyu Zheng</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.03942v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.03942v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Drug discovery is a complex process that involves sequentially screening and examining a vast array of molecules to identify those with the target properties. This process, also referred to as sequential experimentation, faces challenges due to the vast search space, the rarity of target molecules, and constraints imposed by limited data and experimental budgets. To address these challenges, we introduce a human-in-the-loop framework for sequential experiments in drug discovery. This collaborative approach combines human expert knowledge with deep learning algorithms, enhancing the discovery of target molecules within a specified experimental budget. The proposed algorithm processes experimental data to recommend both promising molecules and those that could improve its performance to human experts. Human experts retain the final decision-making authority based on these recommendations and their domain expertise, including the ability to override algorithmic recommendations. We applied our method to drug discovery tasks using real-world data and found that it consistently outperforms all baseline methods, including those which rely solely on human or algorithmic input. This demonstrates the complementarity between human experts and the algorithm. Our results provide key insights into the levels of humans&#39; domain knowledge, the importance of meta-knowledge, and effective work delegation strategies. Our findings suggest that such a framework can significantly accelerate the development of new vaccines and drugs by leveraging the best of both human and artificial intelligence.</p><h4 id="motivating-users-to-attend-to-privacy-a-theory-driven-design-study" tabindex="-1"><a class="header-anchor" href="#motivating-users-to-attend-to-privacy-a-theory-driven-design-study"><span>Motivating Users to Attend to Privacy: A Theory-Driven Design Study</span></a></h4><p><strong>Authors</strong>: Varun Shiri, Maggie Xiong, Jinghui Cheng, Jin L. C. Guo</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.03915v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.03915v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: In modern technology environments, raising users&#39; privacy awareness is crucial. Existing efforts largely focused on privacy policy presentation and failed to systematically address a radical challenge of user motivation for initiating privacy awareness. Leveraging the Protection Motivation Theory (PMT), we proposed design ideas and categories dedicated to motivating users to engage with privacy-related information. Using these design ideas, we created a conceptual prototype, enhancing the current App Store product page. Results from an online experiment and follow-up interviews showed that our design effectively motivated participants to attend to privacy issues, raising both the threat appraisal and coping appraisal, two main factors in PMT. Our work indicated that effective design should consider combining PMT components, calibrating information content, and integrating other design elements, such as visual cues and user familiarity. Overall, our study contributes valuable design considerations driven by the PMT to amplify the motivational aspect of privacy communication.</p><h2 id="_2024-05-06" tabindex="-1"><a class="header-anchor" href="#_2024-05-06"><span>2024-05-06</span></a></h2><h4 id="omniactions-predicting-digital-actions-in-response-to-real-world-multimodal-sensory-inputs-with-llms" tabindex="-1"><a class="header-anchor" href="#omniactions-predicting-digital-actions-in-response-to-real-world-multimodal-sensory-inputs-with-llms"><span>OmniActions: Predicting Digital Actions in Response to Real-World Multimodal Sensory Inputs with LLMs</span></a></h4><p><strong>Authors</strong>: Jiahao Nick Li, Yan Xu, Tovi Grossman, Stephanie Santosa, Michelle Li</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.03901v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.03901v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: The progression to &quot;Pervasive Augmented Reality&quot; envisions easy access to multimodal information continuously. However, in many everyday scenarios, users are occupied physically, cognitively or socially. This may increase the friction to act upon the multimodal information that users encounter in the world. To reduce such friction, future interactive interfaces should intelligently provide quick access to digital actions based on users&#39; context. To explore the range of possible digital actions, we conducted a diary study that required participants to capture and share the media that they intended to perform actions on (e.g., images or audio), along with their desired actions and other contextual information. Using this data, we generated a holistic design space of digital follow-up actions that could be performed in response to different types of multimodal sensory inputs. We then designed OmniActions, a pipeline powered by large language models (LLMs) that processes multimodal sensory inputs and predicts follow-up actions on the target information grounded in the derived design space. Using the empirical data collected in the diary study, we performed quantitative evaluations on three variations of LLM techniques (intent classification, in-context learning and finetuning) and identified the most effective technique for our task. Additionally, as an instantiation of the pipeline, we developed an interactive prototype and reported preliminary user feedback about how people perceive and react to the action predictions and its errors.</p><h4 id="with-or-without-permission-site-specific-augmented-reality-for-social-justice" tabindex="-1"><a class="header-anchor" href="#with-or-without-permission-site-specific-augmented-reality-for-social-justice"><span>With or Without Permission: Site-Specific Augmented Reality for Social Justice</span></a></h4><p><strong>Authors</strong>: Rafael M. L. Silva, Ana María Cárdenas Gasca, Joshua A. Fisher, Erica Principe Cruz, Cinthya Jauregui, Amy Lueck, Fannie Liu, Andrés Monroy-Hernández, Kai Lukoff</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.03898v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.03898v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Movements for social change are often tied to a particular locale. This makes Augmented Reality (AR), which changes how people perceive their surroundings, a promising technology for social justice. Site-specific AR empowers activists to re-tell the story of a place, with or without permission of its owner. It has been used, for example, to reveal hidden histories, re-imagine problematic monuments, and celebrate minority cultures. However, challenges remain concerning technological ownership and accessibility, scalability, sustainability, and navigating collaborations with marginalized communities and across disciplinary boundaries. This half-day workshop at CHI 2024 seeks to bring together an interdisciplinary group of activists, computer scientists, designers, media scholars, and more to identify opportunities and challenges across domains. To anchor the discussion, participants will each share one example of an artifact used in speculating, designing, and/or delivering site-specific AR experiences. This collection of artifacts will inaugurate an interactive database that can inspire a new wave of activists to leverage AR for social justice.</p><h4 id="contextq-generated-questions-to-support-meaningful-parent-child-dialogue-while-co-reading" tabindex="-1"><a class="header-anchor" href="#contextq-generated-questions-to-support-meaningful-parent-child-dialogue-while-co-reading"><span>ContextQ: Generated Questions to Support Meaningful Parent-Child Dialogue While Co-Reading</span></a></h4><p><strong>Authors</strong>: Griffin Dietz Smith, Siddhartha Prasad, Matt J. Davidson, Leah Findlater, R. Benjamin Shapiro</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.03889v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.03889v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Much of early literacy education happens at home with caretakers reading books to young children. Prior research demonstrates how having dialogue with children during co-reading can develop critical reading readiness skills, but most adult readers are unsure if and how to lead effective conversations. We present ContextQ, a tablet-based reading application to unobtrusively present auto-generated dialogic questions to caretakers to support this dialogic reading practice. An ablation study demonstrates how our method of encoding educator expertise into the question generation pipeline can produce high-quality output; and through a user study with 12 parent-child dyads (child age: 4-6), we demonstrate that this system can serve as a guide for parents in leading contextually meaningful dialogue, leading to significantly more conversational turns from both the parent and the child and deeper conversations with connections to the child&#39;s everyday life.</p><h4 id="investigating-personalized-driving-behaviors-in-dilemma-zones-analysis-and-prediction-of-stop-or-go-decisions" tabindex="-1"><a class="header-anchor" href="#investigating-personalized-driving-behaviors-in-dilemma-zones-analysis-and-prediction-of-stop-or-go-decisions"><span>Investigating Personalized Driving Behaviors in Dilemma Zones: Analysis and Prediction of Stop-or-Go Decisions</span></a></h4><p><strong>Authors</strong>: Ziye Qin, Siyan Li, Guoyuan Wu, Matthew J. Barth, Amr Abdelraouf, Rohit Gupta, Kyungtae Han</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.03873v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.03873v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Dilemma zones at signalized intersections present a commonly occurring but unsolved challenge for both drivers and traffic operators. Onsets of the yellow lights prompt varied responses from different drivers: some may brake abruptly, compromising the ride comfort, while others may accelerate, increasing the risk of red-light violations and potential safety hazards. Such diversity in drivers&#39; stop-or-go decisions may result from not only surrounding traffic conditions, but also personalized driving behaviors. To this end, identifying personalized driving behaviors and integrating them into advanced driver assistance systems (ADAS) to mitigate the dilemma zone problem presents an intriguing scientific question. In this study, we employ a game engine-based (i.e., CARLA-enabled) driving simulator to collect high-resolution vehicle trajectories, incoming traffic signal phase and timing information, and stop-or-go decisions from four subject drivers in various scenarios. This approach allows us to analyze personalized driving behaviors in dilemma zones and develop a Personalized Transformer Encoder to predict individual drivers&#39; stop-or-go decisions. The results show that the Personalized Transformer Encoder improves the accuracy of predicting driver decision-making in the dilemma zone by 3.7% to 12.6% compared to the Generic Transformer Encoder, and by 16.8% to 21.6% over the binary logistic regression model.</p><h4 id="enhancing-apparent-personality-trait-analysis-with-cross-modal-embeddings" tabindex="-1"><a class="header-anchor" href="#enhancing-apparent-personality-trait-analysis-with-cross-modal-embeddings"><span>Enhancing Apparent Personality Trait Analysis with Cross-Modal Embeddings</span></a></h4><p><strong>Authors</strong>: Ádám Fodor, Rachid R. Saboundji, András Lőrincz</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.03846v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.03846v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Automatic personality trait assessment is essential for high-quality human-machine interactions. Systems capable of human behavior analysis could be used for self-driving cars, medical research, and surveillance, among many others. We present a multimodal deep neural network with a Siamese extension for apparent personality trait prediction trained on short video recordings and exploiting modality invariant embeddings. Acoustic, visual, and textual information are utilized to reach high-performance solutions in this task. Due to the highly centralized target distribution of the analyzed dataset, the changes in the third digit are relevant. Our proposed method addresses the challenge of under-represented extreme values, achieves 0.0033 MAE average improvement, and shows a clear advantage over the baseline multimodal DNN without the introduced module.</p><h4 id="perception-in-pixels-understanding-avatar-representation-in-video-mediated-collaborative-interactions" tabindex="-1"><a class="header-anchor" href="#perception-in-pixels-understanding-avatar-representation-in-video-mediated-collaborative-interactions"><span>Perception in Pixels: Understanding Avatar Representation in Video-Mediated Collaborative Interactions</span></a></h4><p><strong>Authors</strong>: Pitch Sinlapanuntakul, Mark Zachry</p><p><strong>Link</strong>: <a href="http://arxiv.org/abs/2405.03844v1" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2405.03844v1<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p><strong>Abstract</strong>: Despite the abundance of research concerning virtual reality (VR) avatars, the impact of screen-based or augmented reality (AR) avatars for real-world applications remain relatively unexplored. Notably, there is a lack of research examining video-mediated collaborative interaction experiences using AR avatars for goal-directed group activities. This study bridges this gap with a mixed-methods, quasi-experimental user study that investigates video-based small-group interactions when employing AR avatars as opposed to traditional video for user representation. We found that the use of avatars positively influenced self-esteem and video-based collaboration satisfaction. In addition, our group interview findings highlight experiences and perceptions regarding the dynamic use of avatars in video-mediated collaborative interactions, including benefits, challenges, and factors that would influence a decision to use avatars. This study contributes an empirical understanding of avatar representation in mediating video-based collaborative interactions, implications and perceptions surrounding the adoption of AR avatars, and a comprehensive comparison of key characteristics between user representations.</p></div><!--[--><!--]--></div><footer class="vp-page-meta"><!----><div class="vp-meta-item git-info"><!----><!----></div></footer><!----><!--[--><!--[--><div class="my-footer">MIT Licensed | Copyright © 2024-present 饰三姨——中古饰品</div><!--]--><!--]--></main><!--]--></div><!--[--><!----><!--]--><!--]--></div>
    <script type="module" src="/assets/app-2W_bNIRU.js" defer></script>
  </body>
</html>
