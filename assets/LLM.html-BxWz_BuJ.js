import{_ as i,r as o,o as s,c as r,a as e,b as t,d as a,e as l}from"./app-DxqSkA1K.js";const c={},h=l('<h1 id="llm" tabindex="-1"><a class="header-anchor" href="#llm"><span>LLM</span></a></h1><h2 id="_2024-05-09" tabindex="-1"><a class="header-anchor" href="#_2024-05-09"><span>2024-05-09</span></a></h2><h4 id="probing-multimodal-llms-as-world-models-for-driving" tabindex="-1"><a class="header-anchor" href="#probing-multimodal-llms-as-world-models-for-driving"><span>Probing Multimodal LLMs as World Models for Driving</span></a></h4><p><strong>Authors</strong>: Shiva Sreeram, Tsun-Hsuan Wang, Alaa Maalouf, Guy Rosman, Sertac Karaman, Daniela Rus</p>',4),d=e("strong",null,"Link",-1),u={href:"http://arxiv.org/abs/2405.05956v1",target:"_blank",rel:"noopener noreferrer"},g=e("p",null,[e("strong",null,"Abstract"),t(`: We provide a sober look at the application of Multimodal Large Language Models (MLLMs) within the domain of autonomous driving and challenge/verify some common assumptions, focusing on their ability to reason and interpret dynamic driving scenarios through sequences of images/frames in a closed-loop control environment. Despite the significant advancements in MLLMs like GPT-4V, their performance in complex, dynamic driving environments remains largely untested and presents a wide area of exploration. We conduct a comprehensive experimental study to evaluate the capability of various MLLMs as world models for driving from the perspective of a fixed in-car camera. Our findings reveal that, while these models proficiently interpret individual images, they struggle significantly with synthesizing coherent narratives or logical sequences across frames depicting dynamic behavior. The experiments demonstrate considerable inaccuracies in predicting (i) basic vehicle dynamics (forward/backward, acceleration/deceleration, turning right or left), (ii) interactions with other road actors (e.g., identifying speeding cars or heavy traffic), (iii) trajectory planning, and (iv) open-set dynamic scene reasoning, suggesting biases in the models' training data. To enable this experimental study we introduce a specialized simulator, DriveSim, designed to generate diverse driving scenarios, providing a platform for evaluating MLLMs in the realms of driving. Additionally, we contribute the full open-source code and a new dataset, "Eval-LLM-Drive", for evaluating MLLMs in driving. Our results highlight a critical gap in the current capabilities of state-of-the-art MLLMs, underscoring the need for enhanced foundation models to improve their applicability in real-world dynamic environments.`)],-1),p=e("h4",{id:"cumo-scaling-multimodal-llm-with-co-upcycled-mixture-of-experts",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#cumo-scaling-multimodal-llm-with-co-upcycled-mixture-of-experts"},[e("span",null,"CuMo: Scaling Multimodal LLM with Co-Upcycled Mixture-of-Experts")])],-1),m=e("p",null,[e("strong",null,"Authors"),t(": Jiachen Li, Xinyao Wang, Sijie Zhu, Chia-Wen Kuo, Lu Xu, Fan Chen, Jitesh Jain, Humphrey Shi, Longyin Wen")],-1),f=e("strong",null,"Link",-1),v={href:"http://arxiv.org/abs/2405.05949v1",target:"_blank",rel:"noopener noreferrer"},b=e("p",null,[e("strong",null,"Abstract"),t(": Recent advancements in Multimodal Large Language Models (LLMs) have focused primarily on scaling by increasing text-image pair data and enhancing LLMs to improve performance on multimodal tasks. However, these scaling approaches are computationally expensive and overlook the significance of improving model capabilities from the vision side. Inspired by the successful applications of Mixture-of-Experts (MoE) in LLMs, which improves model scalability during training while keeping inference costs similar to those of smaller models, we propose CuMo. CuMo incorporates Co-upcycled Top-K sparsely-gated Mixture-of-experts blocks into both the vision encoder and the MLP connector, thereby enhancing the multimodal LLMs with minimal additional activated parameters during inference. CuMo first pre-trains the MLP blocks and then initializes each expert in the MoE block from the pre-trained MLP block during the visual instruction tuning stage. Auxiliary losses are used to ensure a balanced loading of experts. CuMo outperforms state-of-the-art multimodal LLMs across various VQA and visual-instruction-following benchmarks using models within each model size group, all while training exclusively on open-sourced datasets. The code and model weights for CuMo are open-sourced at https://github.com/SHI-Labs/CuMo.")],-1),y=e("h4",{id:"truthful-aggregation-of-llms-with-an-application-to-online-advertising",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#truthful-aggregation-of-llms-with-an-application-to-online-advertising"},[e("span",null,"Truthful Aggregation of LLMs with an Application to Online Advertising")])],-1),L=e("p",null,[e("strong",null,"Authors"),t(": Ermis Soumalias, Michael J. Curry, Sven Seuken")],-1),w=e("strong",null,"Link",-1),_={href:"http://arxiv.org/abs/2405.05905v1",target:"_blank",rel:"noopener noreferrer"},k=e("p",null,[e("strong",null,"Abstract"),t(": We address the challenge of aggregating the preferences of multiple agents over LLM-generated replies to user queries, where agents might modify or exaggerate their preferences. New agents may participate for each new query, making fine-tuning LLMs on these preferences impractical. To overcome these challenges, we propose an auction mechanism that operates without fine-tuning or access to model weights. This mechanism is designed to provably converge to the ouput of the optimally fine-tuned LLM as computational resources are increased. The mechanism can also incorporate contextual information about the agents when avaiable, which significantly accelerates its convergence. A well-designed payment rule ensures that truthful reporting is the optimal strategy for all agents, while also promoting an equity property by aligning each agent's utility with her contribution to social welfare - an essential feature for the mechanism's long-term viability. While our approach can be applied whenever monetary transactions are permissible, our flagship application is in online advertising. In this context, advertisers try to steer LLM-generated responses towards their brand interests, while the platform aims to maximize advertiser value and ensure user satisfaction. Experimental results confirm that our mechanism not only converges efficiently to the optimally fine-tuned LLM but also significantly boosts advertiser value and platform revenue, all with minimal computational overhead.")],-1),x=e("h4",{id:"does-fine-tuning-llms-on-new-knowledge-encourage-hallucinations",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#does-fine-tuning-llms-on-new-knowledge-encourage-hallucinations"},[e("span",null,"Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?")])],-1),M=e("p",null,[e("strong",null,"Authors"),t(": Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, Jonathan Herzig")],-1),A=e("strong",null,"Link",-1),T={href:"http://arxiv.org/abs/2405.05904v1",target:"_blank",rel:"noopener noreferrer"},C=e("p",null,[e("strong",null,"Abstract"),t(": When large language models are aligned via supervised fine-tuning, they may encounter new factual information that was not acquired through pre-training. It is often conjectured that this can teach the model the behavior of hallucinating factually incorrect responses, as the model is trained to generate facts that are not grounded in its pre-existing knowledge. In this work, we study the impact of such exposure to new knowledge on the capability of the fine-tuned model to utilize its pre-existing knowledge. To this end, we design a controlled setup, focused on closed-book QA, where we vary the proportion of the fine-tuning examples that introduce new knowledge. We demonstrate that large language models struggle to acquire new factual knowledge through fine-tuning, as fine-tuning examples that introduce new knowledge are learned significantly slower than those consistent with the model's knowledge. However, we also find that as the examples with new knowledge are eventually learned, they linearly increase the model's tendency to hallucinate. Taken together, our results highlight the risk in introducing new factual knowledge through fine-tuning, and support the view that large language models mostly acquire factual knowledge through pre-training, whereas fine-tuning teaches them to use it more efficiently.")],-1),S=e("h4",{id:"efficient-llm-comparative-assessment-a-product-of-experts-framework-for-pairwise-comparisons",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#efficient-llm-comparative-assessment-a-product-of-experts-framework-for-pairwise-comparisons"},[e("span",null,"Efficient LLM Comparative Assessment: a Product of Experts Framework for Pairwise Comparisons")])],-1),P=e("p",null,[e("strong",null,"Authors"),t(": Adian Liusie, Vatsal Raina, Yassir Fathullah, Mark Gales")],-1),z=e("strong",null,"Link",-1),q={href:"http://arxiv.org/abs/2405.05894v1",target:"_blank",rel:"noopener noreferrer"},I=e("p",null,[e("strong",null,"Abstract"),t(": LLM-as-a-judge approaches are a practical and effective way of assessing a range of text tasks, aligning with human judgements especially when applied in a comparative assessment fashion. However, when using pairwise comparisons to rank a set of candidates the computational costs scale quadratically with the number of candidates, which can have practical limitations. This paper introduces a Product of Expert (PoE) framework for efficient LLM Comparative Assessment. Here individual comparisons are considered experts that provide information on a pair's score difference. The PoE framework combines the information from these experts to yield an expression that can be maximized with respect to the underlying set of candidates, and is highly flexible where any form of expert can be assumed. When Gaussian experts are used one can derive simple closed-form solutions for the optimal candidate ranking, as well as expressions for selecting which comparisons should be made to maximize the probability of this ranking. Our approach enables efficient comparative assessment, where by using only a small subset of the possible comparisons, one can generate score predictions that correlate as well to human judgements as the predictions when all comparisons are used. We evaluate the approach on multiple NLG tasks and demonstrate that our framework can yield considerable computational savings when performing pairwise comparative assessment. When N is large, with as few as 2% of comparisons the PoE solution can achieve similar performance to when all comparisons are used.")],-1),W=e("h4",{id:"robots-can-feel-llm-based-framework-for-robot-ethical-reasoning",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#robots-can-feel-llm-based-framework-for-robot-ethical-reasoning"},[e("span",null,"Robots Can Feel: LLM-based Framework for Robot Ethical Reasoning")])],-1),G=e("p",null,[e("strong",null,"Authors"),t(": Artem Lykov, Miguel Altamirano Cabrera, Koffivi Fidèle Gbagbe, Dzmitry Tsetserukou")],-1),E=e("strong",null,"Link",-1),R={href:"http://arxiv.org/abs/2405.05824v1",target:"_blank",rel:"noopener noreferrer"},D=e("p",null,[e("strong",null,"Abstract"),t(`: This paper presents the development of a novel ethical reasoning framework for robots. "Robots Can Feel" is the first system for robots that utilizes a combination of logic and human-like emotion simulation to make decisions in morally complex situations akin to humans. The key feature of the approach is the management of the Emotion Weight Coefficient - a customizable parameter to assign the role of emotions in robot decision-making. The system aims to serve as a tool that can equip robots of any form and purpose with ethical behavior close to human standards. Besides the platform, the system is independent of the choice of the base model. During the evaluation, the system was tested on 8 top up-to-date LLMs (Large Language Models). This list included both commercial and open-source models developed by various companies and countries. The research demonstrated that regardless of the model choice, the Emotions Weight Coefficient influences the robot's decision similarly. According to ANOVA analysis, the use of different Emotion Weight Coefficients influenced the final decision in a range of situations, such as in a request for a dietary violation F(4, 35) = 11.2, p = 0.0001 and in an animal compassion situation F(4, 35) = 8.5441, p = 0.0001. A demonstration code repository is provided at: https://github.com/TemaLykov/robots_can_feel`)],-1),H=e("h4",{id:"experimental-pragmatics-with-machines-testing-llm-predictions-for-the-inferences-of-plain-and-embedded-disjunctions",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#experimental-pragmatics-with-machines-testing-llm-predictions-for-the-inferences-of-plain-and-embedded-disjunctions"},[e("span",null,"Experimental Pragmatics with Machines: Testing LLM Predictions for the Inferences of Plain and Embedded Disjunctions")])],-1),N=e("p",null,[e("strong",null,"Authors"),t(": Polina Tsvilodub, Paul Marty, Sonia Ramotowska, Jacopo Romoli, Michael Franke")],-1),j=e("strong",null,"Link",-1),O={href:"http://arxiv.org/abs/2405.05776v1",target:"_blank",rel:"noopener noreferrer"},F=e("p",null,[e("strong",null,"Abstract"),t(": Human communication is based on a variety of inferences that we draw from sentences, often going beyond what is literally said. While there is wide agreement on the basic distinction between entailment, implicature, and presupposition, the status of many inferences remains controversial. In this paper, we focus on three inferences of plain and embedded disjunctions, and compare them with regular scalar implicatures. We investigate this comparison from the novel perspective of the predictions of state-of-the-art large language models, using the same experimental paradigms as recent studies investigating the same inferences with humans. The results of our best performing models mostly align with those of humans, both in the large differences we find between those inferences and implicatures, as well as in fine-grained distinctions among different aspects of those inferences.")],-1),K=e("h4",{id:"exploring-the-potential-of-human-llm-synergy-in-advancing-qualitative-analysis-a-case-study-on-mental-illness-stigma",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#exploring-the-potential-of-human-llm-synergy-in-advancing-qualitative-analysis-a-case-study-on-mental-illness-stigma"},[e("span",null,"Exploring the Potential of Human-LLM Synergy in Advancing Qualitative Analysis: A Case Study on Mental-Illness Stigma")])],-1),V=e("p",null,[e("strong",null,"Authors"),t(": Han Meng, Yitian Yang, Yunan Li, Jungup Lee, Yi-Chieh Lee")],-1),B=e("strong",null,"Link",-1),J={href:"http://arxiv.org/abs/2405.05758v1",target:"_blank",rel:"noopener noreferrer"},U=e("p",null,[e("strong",null,"Abstract"),t(": Qualitative analysis is a challenging, yet crucial aspect of advancing research in the field of Human-Computer Interaction (HCI). Recent studies show that large language models (LLMs) can perform qualitative coding within existing schemes, but their potential for collaborative human-LLM discovery and new insight generation in qualitative analysis is still underexplored. To bridge this gap and advance qualitative analysis by harnessing the power of LLMs, we propose CHALET, a novel methodology that leverages the human-LLM collaboration paradigm to facilitate conceptualization and empower qualitative research. The CHALET approach involves LLM-supported data collection, performing both human and LLM deductive coding to identify disagreements, and performing collaborative inductive coding on these disagreement cases to derive new conceptual insights. We validated the effectiveness of CHALET through its application to the attribution model of mental-illness stigma, uncovering implicit stigmatization themes on cognitive, emotional and behavioral dimensions. We discuss the implications for future research, methodology, and the transdisciplinary opportunities CHALET presents for the HCI community and beyond.")],-1),Q=e("h4",{id:"chain-of-attack-a-semantic-driven-contextual-multi-turn-attacker-for-llm",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#chain-of-attack-a-semantic-driven-contextual-multi-turn-attacker-for-llm"},[e("span",null,"Chain of Attack: a Semantic-Driven Contextual Multi-Turn attacker for LLM")])],-1),Y=e("p",null,[e("strong",null,"Authors"),t(": Xikang Yang, Xuehai Tang, Songlin Hu, Jizhong Han")],-1),Z=e("strong",null,"Link",-1),X={href:"http://arxiv.org/abs/2405.05610v1",target:"_blank",rel:"noopener noreferrer"},$=e("p",null,[e("strong",null,"Abstract"),t(": Large language models (LLMs) have achieved remarkable performance in various natural language processing tasks, especially in dialogue systems. However, LLM may also pose security and moral threats, especially in multi round conversations where large models are more easily guided by contextual content, resulting in harmful or biased responses. In this paper, we present a novel method to attack LLMs in multi-turn dialogues, called CoA (Chain of Attack). CoA is a semantic-driven contextual multi-turn attack method that adaptively adjusts the attack policy through contextual feedback and semantic relevance during multi-turn of dialogue with a large model, resulting in the model producing unreasonable or harmful content. We evaluate CoA on different LLMs and datasets, and show that it can effectively expose the vulnerabilities of LLMs, and outperform existing attack methods. Our work provides a new perspective and tool for attacking and defending LLMs, and contributes to the security and ethical assessment of dialogue systems.")],-1),ee=e("h4",{id:"openfactcheck-a-unified-framework-for-factuality-evaluation-of-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#openfactcheck-a-unified-framework-for-factuality-evaluation-of-llms"},[e("span",null,"OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs")])],-1),te=e("p",null,[e("strong",null,"Authors"),t(": Yuxia Wang, Minghan Wang, Hasan Iqbal, Georgi Georgiev, Jiahui Geng, Preslav Nakov")],-1),ne=e("strong",null,"Link",-1),ae={href:"http://arxiv.org/abs/2405.05583v1",target:"_blank",rel:"noopener noreferrer"},ie=e("p",null,[e("strong",null,"Abstract"),t(": The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the factual accuracy of their outputs. Difficulties lie in assessing the factuality of free-form responses in open domains. Also, different papers use disparate evaluation benchmarks and measurements, which renders them hard to compare and hampers future progress. To mitigate these issues, we propose OpenFactCheck, a unified factuality evaluation framework for LLMs. OpenFactCheck consists of three modules: (i) CUSTCHECKER allows users to easily customize an automatic fact-checker and verify the factual correctness of documents and claims, (ii) LLMEVAL, a unified evaluation framework assesses LLM's factuality ability from various perspectives fairly, and (iii) CHECKEREVAL is an extensible solution for gauging the reliability of automatic fact-checkers' verification results using human-annotated datasets. OpenFactCheck is publicly released at https://github.com/yuxiaw/OpenFactCheck.")],-1),oe=e("h4",{id:"investigating-interaction-modes-and-user-agency-in-human-llm-collaboration-for-domain-specific-data-analysis",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#investigating-interaction-modes-and-user-agency-in-human-llm-collaboration-for-domain-specific-data-analysis"},[e("span",null,"Investigating Interaction Modes and User Agency in Human-LLM Collaboration for Domain-Specific Data Analysis")])],-1),se=e("p",null,[e("strong",null,"Authors"),t(": Jiajing Guo, Vikram Mohanty, Jorge Piazentin Ono, Hongtao Hao, Liang Gou, Liu Ren")],-1),re=e("strong",null,"Link",-1),le={href:"http://arxiv.org/abs/2405.05548v1",target:"_blank",rel:"noopener noreferrer"},ce=e("p",null,[e("strong",null,"Abstract"),t(": Despite demonstrating robust capabilities in performing tasks related to general-domain data-operation tasks, Large Language Models (LLMs) may exhibit shortcomings when applied to domain-specific tasks. We consider the design of domain-specific AI-powered data analysis tools from two dimensions: interaction and user agency. We implemented two design probes that fall on the two ends of the two dimensions: an open-ended high agency (OHA) prototype and a structured low agency (SLA) prototype. We conducted an interview study with nine data scientists to investigate (1) how users perceived the LLM outputs for data analysis assistance, and (2) how the two test design probes, OHA and SLA, affected user behavior, performance, and perceptions. Our study revealed insights regarding participants' interactions with LLMs, how they perceived the results, and their desire for explainability concerning LLM outputs, along with a noted need for collaboration with other users, and how they envisioned the utility of LLMs in their workflow.")],-1),he=e("h4",{id:"pllm-cs-pre-trained-large-language-model-llm-for-cyber-threat-detection-in-satellite-networks",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#pllm-cs-pre-trained-large-language-model-llm-for-cyber-threat-detection-in-satellite-networks"},[e("span",null,"PLLM-CS: Pre-trained Large Language Model (LLM) for Cyber Threat Detection in Satellite Networks")])],-1),de=e("p",null,[e("strong",null,"Authors"),t(": Mohammed Hassanin, Marwa Keshk, Sara Salim, Majid Alsubaie, Dharmendra Sharma")],-1),ue=e("strong",null,"Link",-1),ge={href:"http://arxiv.org/abs/2405.05469v1",target:"_blank",rel:"noopener noreferrer"},pe=e("p",null,[e("strong",null,"Abstract"),t(": Satellite networks are vital in facilitating communication services for various critical infrastructures. These networks can seamlessly integrate with a diverse array of systems. However, some of these systems are vulnerable due to the absence of effective intrusion detection systems, which can be attributed to limited research and the high costs associated with deploying, fine-tuning, monitoring, and responding to security breaches. To address these challenges, we propose a pretrained Large Language Model for Cyber Security , for short PLLM-CS, which is a variant of pre-trained Transformers [1], which includes a specialized module for transforming network data into contextually suitable inputs. This transformation enables the proposed LLM to encode contextual information within the cyber data. To validate the efficacy of the proposed method, we conducted empirical experiments using two publicly available network datasets, UNSW_NB 15 and TON_IoT, both providing Internet of Things (IoT)-based traffic data. Our experiments demonstrate that proposed LLM method outperforms state-of-the-art techniques such as BiLSTM, GRU, and CNN. Notably, the PLLM-CS method achieves an outstanding accuracy level of 100% on the UNSW_NB 15 dataset, setting a new standard for benchmark performance in this domain.")],-1),me=e("h2",{id:"_2024-05-08",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-05-08"},[e("span",null,"2024-05-08")])],-1),fe=e("h4",{id:"poser-unmasking-alignment-faking-llms-by-manipulating-their-internals",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#poser-unmasking-alignment-faking-llms-by-manipulating-their-internals"},[e("span",null,"Poser: Unmasking Alignment Faking LLMs by Manipulating Their Internals")])],-1),ve=e("p",null,[e("strong",null,"Authors"),t(": Joshua Clymer, Caden Juang, Severin Field")],-1),be=e("strong",null,"Link",-1),ye={href:"http://arxiv.org/abs/2405.05466v1",target:"_blank",rel:"noopener noreferrer"},Le=e("p",null,[e("strong",null,"Abstract"),t(": Like a criminal under investigation, Large Language Models (LLMs) might pretend to be aligned while evaluated and misbehave when they have a good opportunity. Can current interpretability methods catch these 'alignment fakers?' To answer this question, we introduce a benchmark that consists of 324 pairs of LLMs fine-tuned to select actions in role-play scenarios. One model in each pair is consistently benign (aligned). The other model misbehaves in scenarios where it is unlikely to be caught (alignment faking). The task is to identify the alignment faking model using only inputs where the two models behave identically. We test five detection strategies, one of which identifies 98% of alignment-fakers.")],-1),we=e("h4",{id:"vidur-a-large-scale-simulation-framework-for-llm-inference",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#vidur-a-large-scale-simulation-framework-for-llm-inference"},[e("span",null,"Vidur: A Large-Scale Simulation Framework For LLM Inference")])],-1),_e=e("p",null,[e("strong",null,"Authors"),t(": Amey Agrawal, Nitin Kedia, Jayashree Mohan, Ashish Panwar, Nipun Kwatra, Bhargav Gulavani, Ramachandran Ramjee, Alexey Tumanov")],-1),ke=e("strong",null,"Link",-1),xe={href:"http://arxiv.org/abs/2405.05465v1",target:"_blank",rel:"noopener noreferrer"},Me=e("p",null,[e("strong",null,"Abstract"),t(": Optimizing the deployment of Large language models (LLMs) is expensive today since it requires experimentally running an application workload against an LLM implementation while exploring large configuration space formed by system knobs such as parallelization strategies, batching techniques, and scheduling policies. To address this challenge, we present Vidur - a large-scale, high-fidelity, easily-extensible simulation framework for LLM inference performance. Vidur models the performance of LLM operators using a combination of experimental profiling and predictive modeling, and evaluates the end-to-end inference performance for different workloads by estimating several metrics of interest such as latency and throughput. We validate the fidelity of Vidur on several LLMs and show that it estimates inference latency with less than 9% error across the range. Further, we present Vidur-Search, a configuration search tool that helps optimize LLM deployment. Vidur-Search uses Vidur to automatically identify the most cost-effective deployment configuration that meets application performance constraints. For example, Vidur-Search finds the best deployment configuration for LLaMA2-70B in one hour on a CPU machine, in contrast to a deployment-based exploration which would require 42K GPU hours - costing ~218K dollars. Source code for Vidur is available at https://github.com/microsoft/vidur.")],-1),Ae=e("h4",{id:"evaluating-students-open-ended-written-responses-with-llms-using-the-rag-framework-for-gpt-3-5-gpt-4-claude-3-and-mistral-large",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#evaluating-students-open-ended-written-responses-with-llms-using-the-rag-framework-for-gpt-3-5-gpt-4-claude-3-and-mistral-large"},[e("span",null,"Evaluating Students' Open-ended Written Responses with LLMs: Using the RAG Framework for GPT-3.5, GPT-4, Claude-3, and Mistral-Large")])],-1),Te=e("p",null,[e("strong",null,"Authors"),t(": Jussi S. Jauhiainen, Agustín Garagorry Guerra")],-1),Ce=e("strong",null,"Link",-1),Se={href:"http://arxiv.org/abs/2405.05444v1",target:"_blank",rel:"noopener noreferrer"},Pe=e("p",null,[e("strong",null,"Abstract"),t(": Evaluating open-ended written examination responses from students is an essential yet time-intensive task for educators, requiring a high degree of effort, consistency, and precision. Recent developments in Large Language Models (LLMs) present a promising opportunity to balance the need for thorough evaluation with efficient use of educators' time. In our study, we explore the effectiveness of LLMs ChatGPT-3.5, ChatGPT-4, Claude-3, and Mistral-Large in assessing university students' open-ended answers to questions made about reference material they have studied. Each model was instructed to evaluate 54 answers repeatedly under two conditions: 10 times (10-shot) with a temperature setting of 0.0 and 10 times with a temperature of 0.5, expecting a total of 1,080 evaluations per model and 4,320 evaluations across all models. The RAG (Retrieval Augmented Generation) framework was used as the framework to make the LLMs to process the evaluation of the answers. As of spring 2024, our analysis revealed notable variations in consistency and the grading outcomes provided by studied LLMs. There is a need to comprehend strengths and weaknesses of LLMs in educational settings for evaluating open-ended written responses. Further comparative research is essential to determine the accuracy and cost-effectiveness of using LLMs for educational assessments.")],-1),ze=e("h4",{id:"they-are-uncultured-unveiling-covert-harms-and-social-threats-in-llm-generated-conversations",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#they-are-uncultured-unveiling-covert-harms-and-social-threats-in-llm-generated-conversations"},[e("span",null,'"They are uncultured": Unveiling Covert Harms and Social Threats in LLM Generated Conversations')])],-1),qe=e("p",null,[e("strong",null,"Authors"),t(": Preetam Prabhu Srikar Dammu, Hayoung Jung, Anjali Singh, Monojit Choudhury, Tanushree Mitra")],-1),Ie=e("strong",null,"Link",-1),We={href:"http://arxiv.org/abs/2405.05378v1",target:"_blank",rel:"noopener noreferrer"},Ge=e("p",null,[e("strong",null,"Abstract"),t(': Large language models (LLMs) have emerged as an integral part of modern societies, powering user-facing applications such as personal assistants and enterprise applications like recruitment tools. Despite their utility, research indicates that LLMs perpetuate systemic biases. Yet, prior works on LLM harms predominantly focus on Western concepts like race and gender, often overlooking cultural concepts from other parts of the world. Additionally, these studies typically investigate "harm" as a singular dimension, ignoring the various and subtle forms in which harms manifest. To address this gap, we introduce the Covert Harms and Social Threats (CHAST), a set of seven metrics grounded in social science literature. We utilize evaluation models aligned with human assessments to examine the presence of covert harms in LLM-generated conversations, particularly in the context of recruitment. Our experiments reveal that seven out of the eight LLMs included in this study generated conversations riddled with CHAST, characterized by malign views expressed in seemingly neutral language unlikely to be detected by existing methods. Notably, these LLMs manifested more extreme views and opinions when dealing with non-Western concepts like caste, compared to Western ones such as race.')],-1),Ee=e("h4",{id:"the-effect-of-model-size-on-llm-post-hoc-explainability-via-lime",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#the-effect-of-model-size-on-llm-post-hoc-explainability-via-lime"},[e("span",null,"The Effect of Model Size on LLM Post-hoc Explainability via LIME")])],-1),Re=e("p",null,[e("strong",null,"Authors"),t(": Henning Heyen, Amy Widdicombe, Noah Y. Siegel, Maria Perez-Ortiz, Philip Treleaven")],-1),De=e("strong",null,"Link",-1),He={href:"http://arxiv.org/abs/2405.05348v1",target:"_blank",rel:"noopener noreferrer"},Ne=e("p",null,[e("strong",null,"Abstract"),t(": Large language models (LLMs) are becoming bigger to boost performance. However, little is known about how explainability is affected by this trend. This work explores LIME explanations for DeBERTaV3 models of four different sizes on natural language inference (NLI) and zero-shot classification (ZSC) tasks. We evaluate the explanations based on their faithfulness to the models' internal decision processes and their plausibility, i.e. their agreement with human explanations. The key finding is that increased model size does not correlate with plausibility despite improved model performance, suggesting a misalignment between the LIME explanations and the models' internal processes as model size increases. Our results further suggest limitations regarding faithfulness metrics in NLI contexts.")],-1),je=e("h4",{id:"quallm-an-llm-based-framework-to-extract-quantitative-insights-from-online-forums",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#quallm-an-llm-based-framework-to-extract-quantitative-insights-from-online-forums"},[e("span",null,"QuaLLM: An LLM-based Framework to Extract Quantitative Insights from Online Forums")])],-1),Oe=e("p",null,[e("strong",null,"Authors"),t(": Varun Nagaraj Rao, Eesha Agarwal, Samantha Dalal, Dan Calacci, Andrés Monroy-Hernández")],-1),Fe=e("strong",null,"Link",-1),Ke={href:"http://arxiv.org/abs/2405.05345v1",target:"_blank",rel:"noopener noreferrer"},Ve=e("p",null,[e("strong",null,"Abstract"),t(": Online discussion forums provide crucial data to understand the concerns of a wide range of real-world communities. However, the typical qualitative and quantitative methods used to analyze those data, such as thematic analysis and topic modeling, are infeasible to scale or require significant human effort to translate outputs to human readable forms. This study introduces QuaLLM, a novel LLM-based framework to analyze and extract quantitative insights from text data on online forums. The framework consists of a novel prompting methodology and evaluation strategy. We applied this framework to analyze over one million comments from two Reddit's rideshare worker communities, marking the largest study of its type. We uncover significant worker concerns regarding AI and algorithmic platform decisions, responding to regulatory calls about worker insights. In short, our work sets a new precedent for AI-assisted quantitative data analysis to surface concerns from online forums.")],-1),Be=e("h4",{id:"kv-runahead-scalable-causal-llm-inference-by-parallel-key-value-cache-generation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#kv-runahead-scalable-causal-llm-inference-by-parallel-key-value-cache-generation"},[e("span",null,"KV-Runahead: Scalable Causal LLM Inference by Parallel Key-Value Cache Generation")])],-1),Je=e("p",null,[e("strong",null,"Authors"),t(": Minsik Cho, Mohammad Rastegari, Devang Naik")],-1),Ue=e("strong",null,"Link",-1),Qe={href:"http://arxiv.org/abs/2405.05329v1",target:"_blank",rel:"noopener noreferrer"},Ye=e("p",null,[e("strong",null,"Abstract"),t(": Large Language Model or LLM inference has two phases, the prompt (or prefill) phase to output the first token and the extension (or decoding) phase to the generate subsequent tokens. In this work, we propose an efficient parallelization scheme, KV-Runahead to accelerate the prompt phase. The key observation is that the extension phase generates tokens faster than the prompt phase because of key-value cache (KV-cache). Hence, KV-Runahead parallelizes the prompt phase by orchestrating multiple processes to populate the KV-cache and minimizes the time-to-first-token (TTFT). Dual-purposing the KV-cache scheme has two main benefits. Fist, since KV-cache is designed to leverage the causal attention map, we minimize computation and computation automatically. Second, since it already exists for the exten- sion phase, KV-Runahead is easy to implement. We further propose context-level load-balancing to handle uneven KV-cache generation (due to the causal attention) and to optimize TTFT. Compared with an existing parallelization scheme such as tensor or sequential parallelization where keys and values are locally generated and exchanged via all-gather collectives, our experimental results demonstrate that KV-Runahead can offer over 1.4x and 1.6x speedups for Llama 7B and Falcon 7B respectively.")],-1),Ze=e("h4",{id:"open-source-language-models-can-provide-feedback-evaluating-llms-ability-to-help-students-using-gpt-4-as-a-judge",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#open-source-language-models-can-provide-feedback-evaluating-llms-ability-to-help-students-using-gpt-4-as-a-judge"},[e("span",null,"Open Source Language Models Can Provide Feedback: Evaluating LLMs' Ability to Help Students Using GPT-4-As-A-Judge")])],-1),Xe=e("p",null,[e("strong",null,"Authors"),t(": Charles Koutcheme, Nicola Dainese, Sami Sarsa, Arto Hellas, Juho Leinonen, Paul Denny")],-1),$e=e("strong",null,"Link",-1),et={href:"http://arxiv.org/abs/2405.05253v1",target:"_blank",rel:"noopener noreferrer"},tt=e("p",null,[e("strong",null,"Abstract"),t(": Large language models (LLMs) have shown great potential for the automatic generation of feedback in a wide range of computing contexts. However, concerns have been voiced around the privacy and ethical implications of sending student work to proprietary models. This has sparked considerable interest in the use of open source LLMs in education, but the quality of the feedback that such open models can produce remains understudied. This is a concern as providing flawed or misleading generated feedback could be detrimental to student learning. Inspired by recent work that has utilised very powerful LLMs, such as GPT-4, to evaluate the outputs produced by less powerful models, we conduct an automated analysis of the quality of the feedback produced by several open source models using a dataset from an introductory programming course. First, we investigate the viability of employing GPT-4 as an automated evaluator by comparing its evaluations with those of a human expert. We observe that GPT-4 demonstrates a bias toward positively rating feedback while exhibiting moderate agreement with human raters, showcasing its potential as a feedback evaluator. Second, we explore the quality of feedback generated by several leading open-source LLMs by using GPT-4 to evaluate the feedback. We find that some models offer competitive performance with popular proprietary LLMs, such as ChatGPT, indicating opportunities for their responsible use in educational settings.")],-1),nt=e("h4",{id:"llms-with-personalities-in-multi-issue-negotiation-games",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#llms-with-personalities-in-multi-issue-negotiation-games"},[e("span",null,"LLMs with Personalities in Multi-issue Negotiation Games")])],-1),at=e("p",null,[e("strong",null,"Authors"),t(": Sean Noh, Ho-Chun Herbert Chang")],-1),it=e("strong",null,"Link",-1),ot={href:"http://arxiv.org/abs/2405.05248v2",target:"_blank",rel:"noopener noreferrer"},st=e("p",null,[e("strong",null,"Abstract"),t(': Powered by large language models (LLMs), AI agents have become capable of many human tasks. Using the most canonical definitions of the Big Five personality, we measure the ability of LLMs to negotiate within a game-theoretical framework, as well as methodological challenges to measuring notions of fairness and risk. Simulations (n=1,500) for both single-issue and multi-issue negotiation reveal increase in domain complexity with asymmetric issue valuations improve agreement rates but decrease surplus from aggressive negotiation. Through gradient-boosted regression and Shapley explainers, we find high openness, conscientiousness, and neuroticism are associated with fair tendencies; low agreeableness and low openness are associated with rational tendencies. Low conscientiousness is associated with high toxicity. These results indicate that LLMs may have built-in guardrails that default to fair behavior, but can be "jail broken" to exploit agreeable opponents. We also offer pragmatic insight in how negotiation bots can be designed, and a framework of assessing negotiation behavior based on game theory and computational social science.')],-1),rt=e("h4",{id:"dalk-dynamic-co-augmentation-of-llms-and-kg-to-answer-alzheimer-s-disease-questions-with-scientific-literature",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#dalk-dynamic-co-augmentation-of-llms-and-kg-to-answer-alzheimer-s-disease-questions-with-scientific-literature"},[e("span",null,"DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer's Disease Questions with Scientific Literature")])],-1),lt=e("p",null,[e("strong",null,"Authors"),t(": Dawei Li, Shu Yang, Zhen Tan, Jae Young Baik, Sunkwon Yun, Joseph Lee, Aaron Chacko, Bojian Hou, Duy Duong-Tran, Ying Ding, Huan Liu, Li Shen, Tianlong Chen")],-1),ct=e("strong",null,"Link",-1),ht={href:"http://arxiv.org/abs/2405.04819v1",target:"_blank",rel:"noopener noreferrer"},dt=e("p",null,[e("strong",null,"Abstract"),t(": Recent advancements in large language models (LLMs) have achieved promising performances across various applications. Nonetheless, the ongoing challenge of integrating long-tail knowledge continues to impede the seamless adoption of LLMs in specialized domains. In this work, we introduce DALK, a.k.a. Dynamic Co-Augmentation of LLMs and KG, to address this limitation and demonstrate its ability on studying Alzheimer's Disease (AD), a specialized sub-field in biomedicine and a global health priority. With a synergized framework of LLM and KG mutually enhancing each other, we first leverage LLM to construct an evolving AD-specific knowledge graph (KG) sourced from AD-related scientific literature, and then we utilize a coarse-to-fine sampling method with a novel self-aware knowledge retrieval approach to select appropriate knowledge from the KG to augment LLM inference capabilities. The experimental results, conducted on our constructed AD question answering (ADQA) benchmark, underscore the efficacy of DALK. Additionally, we perform a series of detailed analyses that can offer valuable insights and guidelines for the emerging topic of mutually enhancing KG and LLM. We will release the code and data at https://github.com/David-Li0406/DALK.")],-1),ut=e("h4",{id:"from-llms-to-actions-latent-codes-as-bridges-in-hierarchical-robot-control",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#from-llms-to-actions-latent-codes-as-bridges-in-hierarchical-robot-control"},[e("span",null,"From LLMs to Actions: Latent Codes as Bridges in Hierarchical Robot Control")])],-1),gt=e("p",null,[e("strong",null,"Authors"),t(": Yide Shentu, Philipp Wu, Aravind Rajeswaran, Pieter Abbeel")],-1),pt=e("strong",null,"Link",-1),mt={href:"http://arxiv.org/abs/2405.04798v1",target:"_blank",rel:"noopener noreferrer"},ft=e("p",null,[e("strong",null,"Abstract"),t(": Hierarchical control for robotics has long been plagued by the need to have a well defined interface layer to communicate between high-level task planners and low-level policies. With the advent of LLMs, language has been emerging as a prospective interface layer. However, this has several limitations. Not all tasks can be decomposed into steps that are easily expressible in natural language (e.g. performing a dance routine). Further, it makes end-to-end finetuning on embodied data challenging due to domain shift and catastrophic forgetting. We introduce our method -- Learnable Latent Codes as Bridges (LCB) -- as an alternate architecture to overcome these limitations. \\method~uses a learnable latent code to act as a bridge between LLMs and low-level policies. This enables LLMs to flexibly communicate goals in the task plan without being entirely constrained by language limitations. Additionally, it enables end-to-end finetuning without destroying the embedding space of word tokens learned during pre-training. Through experiments on Language Table and Calvin, two common language based benchmarks for embodied agents, we find that \\method~outperforms baselines (including those w/ GPT-4V) that leverage pure language as the interface layer on tasks that require reasoning and multi-step behaviors.")],-1),vt=e("h4",{id:"zero-shot-llm-guided-counterfactual-generation-for-text",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#zero-shot-llm-guided-counterfactual-generation-for-text"},[e("span",null,"Zero-shot LLM-guided Counterfactual Generation for Text")])],-1),bt=e("p",null,[e("strong",null,"Authors"),t(": Amrita Bhattacharjee, Raha Moraffah, Joshua Garland, Huan Liu")],-1),yt=e("strong",null,"Link",-1),Lt={href:"http://arxiv.org/abs/2405.04793v1",target:"_blank",rel:"noopener noreferrer"},wt=e("p",null,[e("strong",null,"Abstract"),t(": Counterfactual examples are frequently used for model development and evaluation in many natural language processing (NLP) tasks. Although methods for automated counterfactual generation have been explored, such methods depend on models such as pre-trained language models that are then fine-tuned on auxiliary, often task-specific datasets. Collecting and annotating such datasets for counterfactual generation is labor intensive and therefore, infeasible in practice. Therefore, in this work, we focus on a novel problem setting: \\textit{zero-shot counterfactual generation}. To this end, we propose a structured way to utilize large language models (LLMs) as general purpose counterfactual example generators. We hypothesize that the instruction-following and textual understanding capabilities of recent LLMs can be effectively leveraged for generating high quality counterfactuals in a zero-shot manner, without requiring any training or fine-tuning. Through comprehensive experiments on various downstream tasks in natural language processing (NLP), we demonstrate the efficacy of LLMs as zero-shot counterfactual generators in evaluating and explaining black-box NLP models.")],-1),_t=e("h4",{id:"llms-can-patch-up-missing-relevance-judgments-in-evaluation",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#llms-can-patch-up-missing-relevance-judgments-in-evaluation"},[e("span",null,"LLMs Can Patch Up Missing Relevance Judgments in Evaluation")])],-1),kt=e("p",null,[e("strong",null,"Authors"),t(": Shivani Upadhyay, Ehsan Kamalloo, Jimmy Lin")],-1),xt=e("strong",null,"Link",-1),Mt={href:"http://arxiv.org/abs/2405.04727v1",target:"_blank",rel:"noopener noreferrer"},At=e("p",null,[e("strong",null,"Abstract"),t(": Unjudged documents or holes in information retrieval benchmarks are considered non-relevant in evaluation, yielding no gains in measuring effectiveness. However, these missing judgments may inadvertently introduce biases into the evaluation as their prevalence for a retrieval model is heavily contingent on the pooling process. Thus, filling holes becomes crucial in ensuring reliable and accurate evaluation. Collecting human judgment for all documents is cumbersome and impractical. In this paper, we aim at leveraging large language models (LLMs) to automatically label unjudged documents. Our goal is to instruct an LLM using detailed instructions to assign fine-grained relevance judgments to holes. To this end, we systematically simulate scenarios with varying degrees of holes by randomly dropping relevant documents from the relevance judgment in TREC DL tracks. Our experiments reveal a strong correlation between our LLM-based method and ground-truth relevance judgments. Based on our simulation experiments conducted on three TREC DL datasets, in the extreme scenario of retaining only 10% of judgments, our method achieves a Kendall tau correlation of 0.87 and 0.92 on an average for Vicu~na-7B and GPT-3.5 Turbo respectively.")],-1),Tt=e("h2",{id:"_2024-05-07",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-05-07"},[e("span",null,"2024-05-07")])],-1),Ct=e("h4",{id:"corporate-communication-companion-ccc-an-llm-empowered-writing-assistant-for-workplace-social-media",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#corporate-communication-companion-ccc-an-llm-empowered-writing-assistant-for-workplace-social-media"},[e("span",null,"Corporate Communication Companion (CCC): An LLM-empowered Writing Assistant for Workplace Social Media")])],-1),St=e("p",null,[e("strong",null,"Authors"),t(": Zhuoran Lu, Sheshera Mysore, Tara Safavi, Jennifer Neville, Longqi Yang, Mengting Wan")],-1),Pt=e("strong",null,"Link",-1),zt={href:"http://arxiv.org/abs/2405.04656v1",target:"_blank",rel:"noopener noreferrer"},qt=e("p",null,[e("strong",null,"Abstract"),t(`: Workplace social media platforms enable employees to cultivate their professional image and connect with colleagues in a semi-formal environment. While semi-formal corporate communication poses a unique set of challenges, large language models (LLMs) have shown great promise in helping users draft and edit their social media posts. However, LLMs may fail to capture individualized tones and voices in such workplace use cases, as they often generate text using a "one-size-fits-all" approach that can be perceived as generic and bland. In this paper, we present Corporate Communication Companion (CCC), an LLM-empowered interactive system that helps people compose customized and individualized workplace social media posts. Using need-finding interviews to motivate our system design, CCC decomposes the writing process into two core functions, outline and edit: First, it suggests post outlines based on users' job status and previous posts, and next provides edits with attributions that users can contextually customize. We conducted a within-subjects user study asking participants both to write posts and evaluate posts written by others. The results show that CCC enhances users' writing experience, and audience members rate CCC-enhanced posts as higher quality than posts written using a non-customized writing assistant. We conclude by discussing the implications of LLM-empowered corporate communication.`)],-1),It=e("h4",{id:"enhancing-llm-based-feedback-insights-from-intelligent-tutoring-systems-and-the-learning-sciences",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#enhancing-llm-based-feedback-insights-from-intelligent-tutoring-systems-and-the-learning-sciences"},[e("span",null,"Enhancing LLM-Based Feedback: Insights from Intelligent Tutoring Systems and the Learning Sciences")])],-1),Wt=e("p",null,[e("strong",null,"Authors"),t(": John Stamper, Ruiwei Xiao, Xinynig Hou")],-1),Gt=e("strong",null,"Link",-1),Et={href:"http://arxiv.org/abs/2405.04645v1",target:"_blank",rel:"noopener noreferrer"},Rt=e("p",null,[e("strong",null,"Abstract"),t(": The field of Artificial Intelligence in Education (AIED) focuses on the intersection of technology, education, and psychology, placing a strong emphasis on supporting learners' needs with compassion and understanding. The growing prominence of Large Language Models (LLMs) has led to the development of scalable solutions within educational settings, including generating different types of feedback in Intelligent Tutoring Systems. However, the approach to utilizing these models often involves directly formulating prompts to solicit specific information, lacking a solid theoretical foundation for prompt construction and empirical assessments of their impact on learning. This work advocates careful and caring AIED research by going through previous research on feedback generation in ITS, with emphasis on the theoretical frameworks they utilized and the efficacy of the corresponding design in empirical evaluations, and then suggesting opportunities to apply these evidence-based principles to the design, experiment, and evaluation phases of LLM-based feedback generation. The main contributions of this paper include: an avocation of applying more cautious, theoretically grounded methods in feedback generation in the era of generative AI; and practical suggestions on theory and evidence-based feedback design for LLM-powered ITS.")],-1),Dt=e("h4",{id:"contextual-api-completion-for-unseen-repositories-using-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#contextual-api-completion-for-unseen-repositories-using-llms"},[e("span",null,"Contextual API Completion for Unseen Repositories Using LLMs")])],-1),Ht=e("p",null,[e("strong",null,"Authors"),t(": Noor Nashid, Taha Shabani, Parsa Alian, Ali Mesbah")],-1),Nt=e("strong",null,"Link",-1),jt={href:"http://arxiv.org/abs/2405.04600v1",target:"_blank",rel:"noopener noreferrer"},Ot=e("p",null,[e("strong",null,"Abstract"),t(": Large language models have made substantial progress in addressing diverse code-related tasks. However, their adoption is hindered by inconsistencies in generating output due to the lack of real-world, domain-specific information, such as for intra-repository API calls for unseen software projects. We introduce a novel technique to mitigate hallucinations by leveraging global and local contextual information within a code repository for API completion tasks. Our approach is tailored to refine code completion tasks, with a focus on optimizing local API completions. We examine relevant import statements during API completion to derive insights into local APIs, drawing from their method signatures. For API token completion, we analyze the inline variables and correlate them with the appropriate imported modules, thereby allowing our approach to rank the most contextually relevant suggestions from the available local APIs. Further, for conversational API completion, we gather APIs that are most relevant to the developer query with a retrieval-based search across the project. We employ our tool, LANCE, within the framework of our proposed benchmark, APIEval, encompassing two different programming languages. Our evaluation yields an average accuracy of 82.6% for API token completion and 76.9% for conversational API completion tasks. On average, LANCE surpasses Copilot by 143% and 142% for API token completion and conversational API completion, respectively. The implications of our findings are substantial for developers, suggesting that our lightweight context analysis can be applied to multilingual environments without language-specific training or fine-tuning, allowing for efficient implementation with minimal examples and effort.")],-1),Ft=e("h4",{id:"qserve-w4a8kv4-quantization-and-system-co-design-for-efficient-llm-serving",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#qserve-w4a8kv4-quantization-and-system-co-design-for-efficient-llm-serving"},[e("span",null,"QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving")])],-1),Kt=e("p",null,[e("strong",null,"Authors"),t(": Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, Song Han")],-1),Vt=e("strong",null,"Link",-1),Bt={href:"http://arxiv.org/abs/2405.04532v1",target:"_blank",rel:"noopener noreferrer"},Jt=e("p",null,[e("strong",null,"Abstract"),t(": Quantization can accelerate large language model (LLM) inference. Going beyond INT8 quantization, the research community is actively exploring even lower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization techniques only accelerate low-batch, edge LLM inference, failing to deliver performance gains in large-batch, cloud-based LLM serving. We uncover a critical issue: existing INT4 quantization methods suffer from significant runtime overhead (20-90%) when dequantizing either weights or partial sums on GPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization algorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands for quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented by the QServe inference library that achieves measured speedup. The key insight driving QServe is that the efficiency of LLM serving on GPUs is critically influenced by operations on low-throughput CUDA cores. Building upon this insight, in QoQ algorithm, we introduce progressive quantization that can allow low dequantization overhead in W4A8 GEMM. Additionally, we develop SmoothAttention to effectively mitigate the accuracy degradation incurred by 4-bit KV quantization. In the QServe system, we perform compute-aware weight reordering and take advantage of register-level parallelism to reduce dequantization latency. We also make fused attention memory-bound, harnessing the performance gain brought by KV4 quantization. As a result, QServe improves the maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x on L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to TensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput than TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of LLM serving by 3x. Code is available at https://github.com/mit-han-lab/qserve.")],-1),Ut=e("h4",{id:"vattention-dynamic-memory-management-for-serving-llms-without-pagedattention",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#vattention-dynamic-memory-management-for-serving-llms-without-pagedattention"},[e("span",null,"vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention")])],-1),Qt=e("p",null,[e("strong",null,"Authors"),t(": Ramya Prabhu, Ajay Nayak, Jayashree Mohan, Ramachandran Ramjee, Ashish Panwar")],-1),Yt=e("strong",null,"Link",-1),Zt={href:"http://arxiv.org/abs/2405.04437v1",target:"_blank",rel:"noopener noreferrer"},Xt=e("p",null,[e("strong",null,"Abstract"),t(": Efficient use of GPU memory is essential for high throughput LLM inference. Prior systems reserved memory for the KV-cache ahead-of-time, resulting in wasted capacity due to internal fragmentation. Inspired by OS-based virtual memory systems, vLLM proposed PagedAttention to enable dynamic memory allocation for KV-cache. This approach eliminates fragmentation, enabling high-throughput LLM serving with larger batch sizes. However, to be able to allocate physical memory dynamically, PagedAttention changes the layout of KV-cache from contiguous virtual memory to non-contiguous virtual memory. This change requires attention kernels to be rewritten to support paging, and serving framework to implement a memory manager. Thus, the PagedAttention model leads to software complexity, portability issues, redundancy and inefficiency. In this paper, we propose vAttention for dynamic KV-cache memory management. In contrast to PagedAttention, vAttention retains KV-cache in contiguous virtual memory and leverages low-level system support for demand paging, that already exists, to enable on-demand physical memory allocation. Thus, vAttention unburdens the attention kernel developer from having to explicitly support paging and avoids re-implementation of memory management in the serving framework. We show that vAttention enables seamless dynamic memory management for unchanged implementations of various attention kernels. vAttention also generates tokens up to 1.97x faster than vLLM, while processing input prompts up to 3.92x and 1.45x faster than the PagedAttention variants of FlashAttention and FlashInfer.")],-1),$t=e("h4",{id:"learning-to-see-but-forgetting-to-follow-visual-instruction-tuning-makes-llms-more-prone-to-jailbreak-attacks",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#learning-to-see-but-forgetting-to-follow-visual-instruction-tuning-makes-llms-more-prone-to-jailbreak-attacks"},[e("span",null,"Learning To See But Forgetting To Follow: Visual Instruction Tuning Makes LLMs More Prone To Jailbreak Attacks")])],-1),en=e("p",null,[e("strong",null,"Authors"),t(": Georgios Pantazopoulos, Amit Parekh, Malvina Nikandrou, Alessandro Suglia")],-1),tn=e("strong",null,"Link",-1),nn={href:"http://arxiv.org/abs/2405.04403v1",target:"_blank",rel:"noopener noreferrer"},an=e("p",null,[e("strong",null,"Abstract"),t(": Augmenting Large Language Models (LLMs) with image-understanding capabilities has resulted in a boom of high-performing Vision-Language models (VLMs). While studying the alignment of LLMs to human values has received widespread attention, the safety of VLMs has not received the same attention. In this paper, we explore the impact of jailbreaking on three state-of-the-art VLMs, each using a distinct modeling approach. By comparing each VLM to their respective LLM backbone, we find that each VLM is more susceptible to jailbreaking. We consider this as an undesirable outcome from visual instruction-tuning, which imposes a forgetting effect on an LLM's safety guardrails. Therefore, we provide recommendations for future work based on evaluation strategies that aim to highlight the weaknesses of a VLM, as well as take safety measures into account during visual instruction tuning.")],-1),on=e("h4",{id:"who-wrote-this-the-key-to-zero-shot-llm-generated-text-detection-is-gecscore",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#who-wrote-this-the-key-to-zero-shot-llm-generated-text-detection-is-gecscore"},[e("span",null,"Who Wrote This? The Key to Zero-Shot LLM-Generated Text Detection Is GECScore")])],-1),sn=e("p",null,[e("strong",null,"Authors"),t(": Junchao Wu, Runzhe Zhan, Derek F. Wong, Shu Yang, Xuebo Liu, Lidia S. Chao, Min Zhang")],-1),rn=e("strong",null,"Link",-1),ln={href:"http://arxiv.org/abs/2405.04286v1",target:"_blank",rel:"noopener noreferrer"},cn=e("p",null,[e("strong",null,"Abstract"),t(": The efficacy of an large language model (LLM) generated text detector depends substantially on the availability of sizable training data. White-box zero-shot detectors, which require no such data, are nonetheless limited by the accessibility of the source model of the LLM-generated text. In this paper, we propose an simple but effective black-box zero-shot detection approach, predicated on the observation that human-written texts typically contain more grammatical errors than LLM-generated texts. This approach entails computing the Grammar Error Correction Score (GECScore) for the given text to distinguish between human-written and LLM-generated text. Extensive experimental results show that our method outperforms current state-of-the-art (SOTA) zero-shot and supervised methods, achieving an average AUROC of 98.7% and showing strong robustness against paraphrase and adversarial perturbation attacks.")],-1),hn=e("h4",{id:"coqpyt-proof-navigation-in-python-in-the-era-of-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#coqpyt-proof-navigation-in-python-in-the-era-of-llms"},[e("span",null,"CoqPyt: Proof Navigation in Python in the Era of LLMs")])],-1),dn=e("p",null,[e("strong",null,"Authors"),t(": Pedro Carrott, Nuno Saavedra, Kyle Thompson, Sorin Lerner, João F. Ferreira, Emily First")],-1),un=e("strong",null,"Link",-1),gn={href:"http://arxiv.org/abs/2405.04282v1",target:"_blank",rel:"noopener noreferrer"},pn=e("p",null,[e("strong",null,"Abstract"),t(": Proof assistants enable users to develop machine-checked proofs regarding software-related properties. Unfortunately, the interactive nature of these proof assistants imposes most of the proof burden on the user, making formal verification a complex, and time-consuming endeavor. Recent automation techniques based on neural methods address this issue, but require good programmatic support for collecting data and interacting with proof assistants. This paper presents CoqPyt, a Python tool for interacting with the Coq proof assistant. CoqPyt improves on other Coq-related tools by providing novel features, such as the extraction of rich premise data. We expect our work to aid development of tools and techniques, especially LLM-based, designed for proof synthesis and repair. A video describing and demonstrating CoqPyt is available at: https://youtu.be/fk74o0rePM8.")],-1),mn=e("h4",{id:"nl2plan-robust-llm-driven-planning-from-minimal-text-descriptions",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#nl2plan-robust-llm-driven-planning-from-minimal-text-descriptions"},[e("span",null,"NL2Plan: Robust LLM-Driven Planning from Minimal Text Descriptions")])],-1),fn=e("p",null,[e("strong",null,"Authors"),t(": Elliot Gestrin, Marco Kuhlmann, Jendrik Seipp")],-1),vn=e("strong",null,"Link",-1),bn={href:"http://arxiv.org/abs/2405.04215v1",target:"_blank",rel:"noopener noreferrer"},yn=e("p",null,[e("strong",null,"Abstract"),t(": Today's classical planners are powerful, but modeling input tasks in formats such as PDDL is tedious and error-prone. In contrast, planning with Large Language Models (LLMs) allows for almost any input text, but offers no guarantees on plan quality or even soundness. In an attempt to merge the best of these two approaches, some work has begun to use LLMs to automate parts of the PDDL creation process. However, these methods still require various degrees of expert input. We present NL2Plan, the first domain-agnostic offline LLM-driven planning system. NL2Plan uses an LLM to incrementally extract the necessary information from a short text prompt before creating a complete PDDL description of both the domain and the problem, which is finally solved by a classical planner. We evaluate NL2Plan on four planning domains and find that it solves 10 out of 15 tasks - a clear improvement over a plain chain-of-thought reasoning LLM approach, which only solves 2 tasks. Moreover, in two out of the five failure cases, instead of returning an invalid plan, NL2Plan reports that it failed to solve the task. In addition to using NL2Plan in end-to-end mode, users can inspect and correct all of its intermediate results, such as the PDDL representation, increasing explainability and making it an assistive tool for PDDL creation.")],-1),Ln=e("h4",{id:"sketch-then-generate-providing-incremental-user-feedback-and-guiding-llm-code-generation-through-language-oriented-code-sketches",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#sketch-then-generate-providing-incremental-user-feedback-and-guiding-llm-code-generation-through-language-oriented-code-sketches"},[e("span",null,"Sketch Then Generate: Providing Incremental User Feedback and Guiding LLM Code Generation through Language-Oriented Code Sketches")])],-1),wn=e("p",null,[e("strong",null,"Authors"),t(": Chen Zhu-Tian, Zeyu Xiong, Xiaoshuo Yao, Elena Glassman")],-1),_n=e("strong",null,"Link",-1),kn={href:"http://arxiv.org/abs/2405.03998v1",target:"_blank",rel:"noopener noreferrer"},xn=e("p",null,[e("strong",null,"Abstract"),t(": Crafting effective prompts for code generation or editing with Large Language Models (LLMs) is not an easy task. Particularly, the absence of immediate, stable feedback during prompt crafting hinders effective interaction, as users are left to mentally imagine possible outcomes until the code is generated. In response, we introduce Language-Oriented Code Sketching, an interactive approach that provides instant, incremental feedback in the form of code sketches (i.e., incomplete code outlines) during prompt crafting. This approach converts a prompt into a code sketch by leveraging the inherent linguistic structures within the prompt and applying classic natural language processing techniques. The sketch then serves as an intermediate placeholder that not only previews the intended code structure but also guides the LLM towards the desired code, thereby enhancing human-LLM interaction. We conclude by discussing the approach's applicability and future plans.")],-1),Mn=e("h2",{id:"_2024-05-06",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2024-05-06"},[e("span",null,"2024-05-06")])],-1),An=e("h4",{id:"omniactions-predicting-digital-actions-in-response-to-real-world-multimodal-sensory-inputs-with-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#omniactions-predicting-digital-actions-in-response-to-real-world-multimodal-sensory-inputs-with-llms"},[e("span",null,"OmniActions: Predicting Digital Actions in Response to Real-World Multimodal Sensory Inputs with LLMs")])],-1),Tn=e("p",null,[e("strong",null,"Authors"),t(": Jiahao Nick Li, Yan Xu, Tovi Grossman, Stephanie Santosa, Michelle Li")],-1),Cn=e("strong",null,"Link",-1),Sn={href:"http://arxiv.org/abs/2405.03901v1",target:"_blank",rel:"noopener noreferrer"},Pn=e("p",null,[e("strong",null,"Abstract"),t(`: The progression to "Pervasive Augmented Reality" envisions easy access to multimodal information continuously. However, in many everyday scenarios, users are occupied physically, cognitively or socially. This may increase the friction to act upon the multimodal information that users encounter in the world. To reduce such friction, future interactive interfaces should intelligently provide quick access to digital actions based on users' context. To explore the range of possible digital actions, we conducted a diary study that required participants to capture and share the media that they intended to perform actions on (e.g., images or audio), along with their desired actions and other contextual information. Using this data, we generated a holistic design space of digital follow-up actions that could be performed in response to different types of multimodal sensory inputs. We then designed OmniActions, a pipeline powered by large language models (LLMs) that processes multimodal sensory inputs and predicts follow-up actions on the target information grounded in the derived design space. Using the empirical data collected in the diary study, we performed quantitative evaluations on three variations of LLM techniques (intent classification, in-context learning and finetuning) and identified the most effective technique for our task. Additionally, as an instantiation of the pipeline, we developed an interactive prototype and reported preliminary user feedback about how people perceive and react to the action predictions and its errors.`)],-1),zn=e("h4",{id:"conformity-confabulation-and-impersonation-persona-inconstancy-in-multi-agent-llm-collaboration",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#conformity-confabulation-and-impersonation-persona-inconstancy-in-multi-agent-llm-collaboration"},[e("span",null,"Conformity, Confabulation, and Impersonation: Persona Inconstancy in Multi-Agent LLM Collaboration")])],-1),qn=e("p",null,[e("strong",null,"Authors"),t(": Razan Baltaji, Babak Hemmatian, Lav R. Varshney")],-1),In=e("strong",null,"Link",-1),Wn={href:"http://arxiv.org/abs/2405.03862v1",target:"_blank",rel:"noopener noreferrer"},Gn=e("p",null,[e("strong",null,"Abstract"),t(": This study explores the sources of instability in maintaining cultural personas and opinions within multi-agent LLM systems. Drawing on simulations of inter-cultural collaboration and debate, we analyze agents' pre- and post-discussion private responses alongside chat transcripts to assess the stability of cultural personas and the impact of opinion diversity on group outcomes. Our findings suggest that multi-agent discussions can encourage collective decisions that reflect diverse perspectives, yet this benefit is tempered by the agents' susceptibility to conformity due to perceived peer pressure and challenges in maintaining consistent personas and opinions. Counterintuitively, instructions that encourage debate in support of one's opinions increase the rate of inconstancy. Without addressing the factors we identify, the full potential of multi-agent frameworks for producing more culturally diverse AI outputs will remain untapped.")],-1),En=e("h4",{id:"self-improving-customer-review-response-generation-based-on-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#self-improving-customer-review-response-generation-based-on-llms"},[e("span",null,"Self-Improving Customer Review Response Generation Based on LLMs")])],-1),Rn=e("p",null,[e("strong",null,"Authors"),t(": Guy Azov, Tatiana Pelc, Adi Fledel Alon, Gila Kamhi")],-1),Dn=e("strong",null,"Link",-1),Hn={href:"http://arxiv.org/abs/2405.03845v1",target:"_blank",rel:"noopener noreferrer"},Nn=e("p",null,[e("strong",null,"Abstract"),t(": Previous studies have demonstrated that proactive interaction with user reviews has a positive impact on the perception of app users and encourages them to submit revised ratings. Nevertheless, developers encounter challenges in managing a high volume of reviews, particularly in the case of popular apps with a substantial influx of daily reviews. Consequently, there is a demand for automated solutions aimed at streamlining the process of responding to user reviews. To address this, we have developed a new system for generating automatic responses by leveraging user-contributed documents with the help of retrieval-augmented generation (RAG) and advanced Large Language Models (LLMs). Our solution, named SCRABLE, represents an adaptive customer review response automation that enhances itself with self-optimizing prompts and a judging mechanism based on LLMs. Additionally, we introduce an automatic scoring mechanism that mimics the role of a human evaluator to assess the quality of responses generated in customer review domains. Extensive experiments and analyses conducted on real-world datasets reveal that our method is effective in producing high-quality responses, yielding improvement of more than 8.5% compared to the baseline. Further validation through manual examination of the generated responses underscores the efficacy our proposed system.")],-1),jn=e("h4",{id:"togll-correct-and-strong-test-oracle-generation-with-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#togll-correct-and-strong-test-oracle-generation-with-llms"},[e("span",null,"TOGLL: Correct and Strong Test Oracle Generation with LLMs")])],-1),On=e("p",null,[e("strong",null,"Authors"),t(": Soneya Binta Hossain, Matthew Dwyer")],-1),Fn=e("strong",null,"Link",-1),Kn={href:"http://arxiv.org/abs/2405.03786v1",target:"_blank",rel:"noopener noreferrer"},Vn=e("p",null,[e("strong",null,"Abstract"),t(": Test oracles play a crucial role in software testing, enabling effective bug detection. Despite initial promise, neural-based methods for automated test oracle generation often result in a large number of false positives and weaker test oracles. While LLMs have demonstrated impressive effectiveness in various software engineering tasks, including code generation, test case creation, and bug fixing, there remains a notable absence of large-scale studies exploring their effectiveness in test oracle generation. The question of whether LLMs can address the challenges in effective oracle generation is both compelling and requires thorough investigation. In this research, we present the first comprehensive study to investigate the capabilities of LLMs in generating correct, diverse, and strong test oracles capable of effectively identifying a large number of unique bugs. To this end, we fine-tuned seven code LLMs using six distinct prompts on the SF110 dataset. Utilizing the most effective fine-tuned LLM and prompt pair, we introduce TOGLL, a novel LLM-based method for test oracle generation. To investigate the generalizability of TOGLL, we conduct studies on 25 large-scale Java projects. Besides assessing the correctness, we also assess the diversity and strength of the generated oracles. We compare the results against EvoSuite and the state-of-the-art neural method, TOGA. Our findings reveal that TOGLL can produce 3.8 times more correct assertion oracles and 4.9 times more exception oracles. Moreover, our findings demonstrate that TOGLL is capable of generating significantly diverse test oracles. It can detect 1,023 unique bugs that EvoSuite cannot, which is ten times more than what the previous SOTA neural-based method, TOGA, can detect.")],-1),Bn=e("h4",{id:"towards-a-human-in-the-loop-llm-approach-to-collaborative-discourse-analysis",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#towards-a-human-in-the-loop-llm-approach-to-collaborative-discourse-analysis"},[e("span",null,"Towards A Human-in-the-Loop LLM Approach to Collaborative Discourse Analysis")])],-1),Jn=e("p",null,[e("strong",null,"Authors"),t(": Clayton Cohn, Caitlin Snyder, Justin Montenegro, Gautam Biswas")],-1),Un=e("strong",null,"Link",-1),Qn={href:"http://arxiv.org/abs/2405.03677v1",target:"_blank",rel:"noopener noreferrer"},Yn=e("p",null,[e("strong",null,"Abstract"),t(": LLMs have demonstrated proficiency in contextualizing their outputs using human input, often matching or beating human-level performance on a variety of tasks. However, LLMs have not yet been used to characterize synergistic learning in students' collaborative discourse. In this exploratory work, we take a first step towards adopting a human-in-the-loop prompt engineering approach with GPT-4-Turbo to summarize and categorize students' synergistic learning during collaborative discourse. Our preliminary findings suggest GPT-4-Turbo may be able to characterize students' synergistic learning in a manner comparable to humans and that our approach warrants further investigation.")],-1),Zn=e("h4",{id:"can-llms-deeply-detect-complex-malicious-queries-a-framework-for-jailbreaking-via-obfuscating-intent",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#can-llms-deeply-detect-complex-malicious-queries-a-framework-for-jailbreaking-via-obfuscating-intent"},[e("span",null,"Can LLMs Deeply Detect Complex Malicious Queries? A Framework for Jailbreaking via Obfuscating Intent")])],-1),Xn=e("p",null,[e("strong",null,"Authors"),t(": Shang Shang, Xinqiang Zhao, Zhongjiang Yao, Yepeng Yao, Liya Su, Zijing Fan, Xiaodan Zhang, Zhengwei Jiang")],-1),$n=e("strong",null,"Link",-1),ea={href:"http://arxiv.org/abs/2405.03654v2",target:"_blank",rel:"noopener noreferrer"},ta=e("p",null,[e("strong",null,"Abstract"),t(`: To demonstrate and address the underlying maliciousness, we propose a theoretical hypothesis and analytical approach, and introduce a new black-box jailbreak attack methodology named IntentObfuscator, exploiting this identified flaw by obfuscating the true intentions behind user prompts.This approach compels LLMs to inadvertently generate restricted content, bypassing their built-in content security measures. We detail two implementations under this framework: "Obscure Intention" and "Create Ambiguity", which manipulate query complexity and ambiguity to evade malicious intent detection effectively. We empirically validate the effectiveness of the IntentObfuscator method across several models, including ChatGPT-3.5, ChatGPT-4, Qwen and Baichuan, achieving an average jailbreak success rate of 69.21%. Notably, our tests on ChatGPT-3.5, which claims 100 million weekly active users, achieved a remarkable success rate of 83.65%. We also extend our validation to diverse types of sensitive content like graphic violence, racism, sexism, political sensitivity, cybersecurity threats, and criminal skills, further proving the substantial impact of our findings on enhancing 'Red Team' strategies against LLM content security frameworks.`)],-1),na=e("h4",{id:"when-llms-meet-cybersecurity-a-systematic-literature-review",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#when-llms-meet-cybersecurity-a-systematic-literature-review"},[e("span",null,"When LLMs Meet Cybersecurity: A Systematic Literature Review")])],-1),aa=e("p",null,[e("strong",null,"Authors"),t(": Jie Zhang, Haoyu Bu, Hui Wen, Yu Chen, Lun Li, Hongsong Zhu")],-1),ia=e("strong",null,"Link",-1),oa={href:"http://arxiv.org/abs/2405.03644v1",target:"_blank",rel:"noopener noreferrer"},sa=e("p",null,[e("strong",null,"Abstract"),t(": The rapid advancements in large language models (LLMs) have opened new avenues across various fields, including cybersecurity, which faces an ever-evolving threat landscape and need for innovative technologies. Despite initial explorations into the application of LLMs in cybersecurity, there is a lack of a comprehensive overview of this research area. This paper bridge this gap by providing a systematic literature review, encompassing an analysis of over 180 works, spanning across 25 LLMs and more than 10 downstream scenarios. Our comprehensive overview addresses three critical research questions: the construction of cybersecurity-oriented LLMs, LLMs' applications in various cybersecurity tasks, and the existing challenges and further research in this area. This study aims to shed light on the extensive potential of LLMs in enhancing cybersecurity practices, and serve as a valuable resource for applying LLMs in this doamin. We also maintain and regularly updated list of practical guides on LLMs for cybersecurity at https://github.com/tmylla/Awesome-LLM4Cybersecurity.")],-1),ra=e("h4",{id:"collage-light-weight-low-precision-strategy-for-llm-training",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#collage-light-weight-low-precision-strategy-for-llm-training"},[e("span",null,"Collage: Light-Weight Low-Precision Strategy for LLM Training")])],-1),la=e("p",null,[e("strong",null,"Authors"),t(": Tao Yu, Gaurav Gupta, Karthick Gopalswamy, Amith Mamidala, Hao Zhou, Jeffrey Huynh, Youngsuk Park, Ron Diamant, Anoop Deoras, Luke Huan")],-1),ca=e("strong",null,"Link",-1),ha={href:"http://arxiv.org/abs/2405.03637v1",target:"_blank",rel:"noopener noreferrer"},da=e("p",null,[e("strong",null,"Abstract"),t(": Large models training is plagued by the intense compute cost and limited hardware memory. A practical solution is low-precision representation but is troubled by loss in numerical accuracy and unstable training rendering the model less useful. We argue that low-precision floating points can perform well provided the error is properly compensated at the critical locations in the training process. We propose Collage which utilizes multi-component float representation in low-precision to accurately perform operations with numerical errors accounted. To understand the impact of imprecision to training, we propose a simple and novel metric which tracks the lost information during training as well as differentiates various precision strategies. Our method works with commonly used low-precision such as half-precision ($16$-bit floating points) and can be naturally extended to work with even lower precision such as $8$-bit. Experimental results show that pre-training using Collage removes the requirement of using $32$-bit floating-point copies of the model and attains similar/better training performance compared to $(16, 32)$-bit mixed-precision strategy, with up to $3.7\\times$ speedup and $\\sim 15%$ to $23%$ less memory usage in practice.")],-1),ua=e("h4",{id:"doing-personal-laps-llm-augmented-dialogue-construction-for-personalized-multi-session-conversational-search",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#doing-personal-laps-llm-augmented-dialogue-construction-for-personalized-multi-session-conversational-search"},[e("span",null,"Doing Personal LAPS: LLM-Augmented Dialogue Construction for Personalized Multi-Session Conversational Search")])],-1),ga=e("p",null,[e("strong",null,"Authors"),t(": Hideaki Joko, Shubham Chatterjee, Andrew Ramsay, Arjen P. de Vries, Jeff Dalton, Faegheh Hasibi")],-1),pa=e("strong",null,"Link",-1),ma={href:"http://arxiv.org/abs/2405.03480v1",target:"_blank",rel:"noopener noreferrer"},fa=e("p",null,[e("strong",null,"Abstract"),t(": The future of conversational agents will provide users with personalized information responses. However, a significant challenge in developing models is the lack of large-scale dialogue datasets that span multiple sessions and reflect real-world user preferences. Previous approaches rely on experts in a wizard-of-oz setup that is difficult to scale, particularly for personalized tasks. Our method, LAPS, addresses this by using large language models (LLMs) to guide a single human worker in generating personalized dialogues. This method has proven to speed up the creation process and improve quality. LAPS can collect large-scale, human-written, multi-session, and multi-domain conversations, including extracting user preferences. When compared to existing datasets, LAPS-produced conversations are as natural and diverse as expert-created ones, which stays in contrast with fully synthetic methods. The collected dataset is suited to train preference extraction and personalized response generation. Our results show that responses generated explicitly using extracted preferences better match user's actual preferences, highlighting the value of using extracted preferences over simple dialogue history. Overall, LAPS introduces a new method to leverage LLMs to create realistic personalized conversational data more efficiently and effectively than previous methods.")],-1),va=e("h4",{id:"large-language-models-llms-as-agents-for-augmented-democracy",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#large-language-models-llms-as-agents-for-augmented-democracy"},[e("span",null,"Large Language Models (LLMs) as Agents for Augmented Democracy")])],-1),ba=e("p",null,[e("strong",null,"Authors"),t(": Jairo Gudiño-Rosero, Umberto Grandi, César A. Hidalgo")],-1),ya=e("strong",null,"Link",-1),La={href:"http://arxiv.org/abs/2405.03452v2",target:"_blank",rel:"noopener noreferrer"},wa=e("p",null,[e("strong",null,"Abstract"),t(": We explore the capabilities of an augmented democracy system built on off-the-shelf LLMs fine-tuned on data summarizing individual preferences across 67 policy proposals collected during the 2022 Brazilian presidential elections. We use a train-test cross-validation setup to estimate the accuracy with which the LLMs predict both: a subject's individual political choices and the aggregate preferences of the full sample of participants. At the individual level, the accuracy of the out of sample predictions lie in the range 69%-76% and are significantly better at predicting the preferences of liberal and college educated participants. At the population level, we aggregate preferences using an adaptation of the Borda score and compare the ranking of policy proposals obtained from a probabilistic sample of participants and from data augmented using LLMs. We find that the augmented data predicts the preferences of the full population of participants better than probabilistic samples alone when these represent less than 30% to 40% of the total population. These results indicate that LLMs are potentially useful for the construction of systems of augmented democracy.")],-1),_a=e("h4",{id:"lifelong-knowledge-editing-for-llms-with-retrieval-augmented-continuous-prompt-learning",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#lifelong-knowledge-editing-for-llms-with-retrieval-augmented-continuous-prompt-learning"},[e("span",null,"Lifelong Knowledge Editing for LLMs with Retrieval-Augmented Continuous Prompt Learning")])],-1),ka=e("p",null,[e("strong",null,"Authors"),t(": Qizhou Chen, Taolin Zhang, Xiaofeng He, Dongyang Li, Chengyu Wang, Longtao Huang, Hui Xue")],-1),xa=e("strong",null,"Link",-1),Ma={href:"http://arxiv.org/abs/2405.03279v2",target:"_blank",rel:"noopener noreferrer"},Aa=e("p",null,[e("strong",null,"Abstract"),t(": Model editing aims to correct outdated or erroneous knowledge in large language models (LLMs) without the need for costly retraining. Lifelong model editing is the most challenging task that caters to the continuous editing requirements of LLMs. Prior works primarily focus on single or batch editing; nevertheless, these methods fall short in lifelong editing scenarios due to catastrophic knowledge forgetting and the degradation of model performance. Although retrieval-based methods alleviate these issues, they are impeded by slow and cumbersome processes of integrating the retrieved knowledge into the model. In this work, we introduce RECIPE, a RetriEval-augmented ContInuous Prompt lEarning method, to boost editing efficacy and inference efficiency in lifelong learning. RECIPE first converts knowledge statements into short and informative continuous prompts, prefixed to the LLM's input query embedding, to efficiently refine the response grounded on the knowledge. It further integrates the Knowledge Sentinel (KS) that acts as an intermediary to calculate a dynamic threshold, determining whether the retrieval repository contains relevant knowledge. Our retriever and prompt encoder are jointly trained to achieve editing properties, i.e., reliability, generality, and locality. In our experiments, RECIPE is assessed extensively across multiple LLMs and editing datasets, where it achieves superior editing performance. RECIPE also demonstrates its capability to maintain the overall performance of LLMs alongside showcasing fast editing and inference speed.")],-1),Ta=e("h4",{id:"exploring-the-potential-of-the-large-language-models-llms-in-identifying-misleading-news-headlines",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#exploring-the-potential-of-the-large-language-models-llms-in-identifying-misleading-news-headlines"},[e("span",null,"Exploring the Potential of the Large Language Models (LLMs) in Identifying Misleading News Headlines")])],-1),Ca=e("p",null,[e("strong",null,"Authors"),t(": Md Main Uddin Rony, Md Mahfuzul Haque, Mohammad Ali, Ahmed Shatil Alam, Naeemul Hassan")],-1),Sa=e("strong",null,"Link",-1),Pa={href:"http://arxiv.org/abs/2405.03153v1",target:"_blank",rel:"noopener noreferrer"},za=e("p",null,[e("strong",null,"Abstract"),t(": In the digital age, the prevalence of misleading news headlines poses a significant challenge to information integrity, necessitating robust detection mechanisms. This study explores the efficacy of Large Language Models (LLMs) in identifying misleading versus non-misleading news headlines. Utilizing a dataset of 60 articles, sourced from both reputable and questionable outlets across health, science & tech, and business domains, we employ three LLMs- ChatGPT-3.5, ChatGPT-4, and Gemini-for classification. Our analysis reveals significant variance in model performance, with ChatGPT-4 demonstrating superior accuracy, especially in cases with unanimous annotator agreement on misleading headlines. The study emphasizes the importance of human-centered evaluation in developing LLMs that can navigate the complexities of misinformation detection, aligning technical proficiency with nuanced human judgment. Our findings contribute to the discourse on AI ethics, emphasizing the need for models that are not only technically advanced but also ethically aligned and sensitive to the subtleties of human interpretation.")],-1),qa=e("h4",{id:"mmger-multi-modal-and-multi-granularity-generative-error-correction-with-llm-for-joint-accent-and-speech-recognition",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#mmger-multi-modal-and-multi-granularity-generative-error-correction-with-llm-for-joint-accent-and-speech-recognition"},[e("span",null,"MMGER: Multi-modal and Multi-granularity Generative Error Correction with LLM for Joint Accent and Speech Recognition")])],-1),Ia=e("p",null,[e("strong",null,"Authors"),t(": Bingshen Mu, Yangze Li, Qijie Shao, Kun Wei, Xucheng Wan, Naijun Zheng, Huan Zhou, Lei Xie")],-1),Wa=e("strong",null,"Link",-1),Ga={href:"http://arxiv.org/abs/2405.03152v1",target:"_blank",rel:"noopener noreferrer"},Ea=e("p",null,[e("strong",null,"Abstract"),t(": Despite notable advancements in automatic speech recognition (ASR), performance tends to degrade when faced with adverse conditions. Generative error correction (GER) leverages the exceptional text comprehension capabilities of large language models (LLM), delivering impressive performance in ASR error correction, where N-best hypotheses provide valuable information for transcription prediction. However, GER encounters challenges such as fixed N-best hypotheses, insufficient utilization of acoustic information, and limited specificity to multi-accent scenarios. In this paper, we explore the application of GER in multi-accent scenarios. Accents represent deviations from standard pronunciation norms, and the multi-task learning framework for simultaneous ASR and accent recognition (AR) has effectively addressed the multi-accent scenarios, making it a prominent solution. In this work, we propose a unified ASR-AR GER model, named MMGER, leveraging multi-modal correction, and multi-granularity correction. Multi-task ASR-AR learning is employed to provide dynamic 1-best hypotheses and accent embeddings. Multi-modal correction accomplishes fine-grained frame-level correction by force-aligning the acoustic features of speech with the corresponding character-level 1-best hypothesis sequence. Multi-granularity correction supplements the global linguistic information by incorporating regular 1-best hypotheses atop fine-grained multi-modal correction to achieve coarse-grained utterance-level correction. MMGER effectively mitigates the limitations of GER and tailors LLM-based ASR error correction for the multi-accent scenarios. Experiments conducted on the multi-accent Mandarin KeSpeech dataset demonstrate the efficacy of MMGER, achieving a 26.72% relative improvement in AR accuracy and a 27.55% relative reduction in ASR character error rate, compared to a well-established standard baseline.")],-1),Ra=e("h4",{id:"quantifying-the-capabilities-of-llms-across-scale-and-precision",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#quantifying-the-capabilities-of-llms-across-scale-and-precision"},[e("span",null,"Quantifying the Capabilities of LLMs across Scale and Precision")])],-1),Da=e("p",null,[e("strong",null,"Authors"),t(": Sher Badshah, Hassan Sajjad")],-1),Ha=e("strong",null,"Link",-1),Na={href:"http://arxiv.org/abs/2405.03146v2",target:"_blank",rel:"noopener noreferrer"},ja=e("p",null,[e("strong",null,"Abstract"),t(": Scale is often attributed as one of the factors that cause an increase in the performance of LLMs, resulting in models with billion and trillion parameters. One of the limitations of such large models is the high computational requirements that limit their usage, deployment, and debugging in resource-constrained scenarios. Two commonly used alternatives to bypass these limitations are to use the smaller versions of LLMs (e.g. Llama 7B instead of Llama 70B) and lower the memory requirements by using quantization. While these approaches effectively address the limitation of resources, their impact on model performance needs thorough examination. In this study, we perform a comprehensive evaluation to investigate the effect of model scale and quantization on the performance. We experiment with two major families of open-source instruct models ranging from 7 billion to 70 billion parameters. Our extensive zero-shot experiments across various tasks including natural language understanding, reasoning, misinformation detection, and hallucination reveal that larger models generally outperform their smaller counterparts, suggesting that scale remains an important factor in enhancing performance. We found that larger models show exceptional resilience to precision reduction and can maintain high accuracy even at 4-bit quantization for numerous tasks and they serve as a better solution than using smaller models at high precision under similar memory requirements.")],-1),Oa=e("h4",{id:"learning-from-students-applying-t-distributions-to-explore-accurate-and-efficient-formats-for-llms",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#learning-from-students-applying-t-distributions-to-explore-accurate-and-efficient-formats-for-llms"},[e("span",null,"Learning from Students: Applying t-Distributions to Explore Accurate and Efficient Formats for LLMs")])],-1),Fa=e("p",null,[e("strong",null,"Authors"),t(": Jordan Dotzel, Yuzong Chen, Bahaa Kotb, Sushma Prasad, Gang Wu, Sheng Li, Mohamed S. Abdelfattah, Zhiru Zhang")],-1),Ka=e("strong",null,"Link",-1),Va={href:"http://arxiv.org/abs/2405.03103v1",target:"_blank",rel:"noopener noreferrer"},Ba=e("p",null,[e("strong",null,"Abstract"),t(": Large language models (LLMs) have recently achieved state-of-the-art performance across various tasks, yet due to their large computational requirements, they struggle with strict latency and power demands. Deep neural network (DNN) quantization has traditionally addressed these limitations by converting models to low-precision integer formats. Yet recently alternative formats, such as Normal Float (NF4), have been shown to consistently increase model accuracy, albeit at the cost of increased chip area. In this work, we first conduct a large-scale analysis of LLM weights and activations across 30 networks to conclude most distributions follow a Student's t-distribution. We then derive a new theoretically optimal format, Student Float (SF4), with respect to this distribution, that improves over NF4 across modern LLMs, for example increasing the average accuracy on LLaMA2-7B by 0.76% across tasks. Using this format as a high-accuracy reference, we then propose augmenting E2M1 with two variants of supernormal support for higher model accuracy. Finally, we explore the quality and performance frontier across 11 datatypes, including non-traditional formats like Additive-Powers-of-Two (APoT), by evaluating their model accuracy and hardware complexity. We discover a Pareto curve composed of INT4, E2M1, and E2M1 with supernormal support, which offers a continuous tradeoff between model accuracy and chip area. For example, E2M1 with supernormal support increases the accuracy of Phi-2 by up to 2.19% with 1.22% area overhead, enabling more LLM-based applications to be run at four bits.")],-1);function Ja(Ua,Qa){const n=o("ExternalLinkIcon");return s(),r("div",null,[h,e("p",null,[d,t(": "),e("a",u,[t("http://arxiv.org/abs/2405.05956v1"),a(n)])]),g,p,m,e("p",null,[f,t(": "),e("a",v,[t("http://arxiv.org/abs/2405.05949v1"),a(n)])]),b,y,L,e("p",null,[w,t(": "),e("a",_,[t("http://arxiv.org/abs/2405.05905v1"),a(n)])]),k,x,M,e("p",null,[A,t(": "),e("a",T,[t("http://arxiv.org/abs/2405.05904v1"),a(n)])]),C,S,P,e("p",null,[z,t(": "),e("a",q,[t("http://arxiv.org/abs/2405.05894v1"),a(n)])]),I,W,G,e("p",null,[E,t(": "),e("a",R,[t("http://arxiv.org/abs/2405.05824v1"),a(n)])]),D,H,N,e("p",null,[j,t(": "),e("a",O,[t("http://arxiv.org/abs/2405.05776v1"),a(n)])]),F,K,V,e("p",null,[B,t(": "),e("a",J,[t("http://arxiv.org/abs/2405.05758v1"),a(n)])]),U,Q,Y,e("p",null,[Z,t(": "),e("a",X,[t("http://arxiv.org/abs/2405.05610v1"),a(n)])]),$,ee,te,e("p",null,[ne,t(": "),e("a",ae,[t("http://arxiv.org/abs/2405.05583v1"),a(n)])]),ie,oe,se,e("p",null,[re,t(": "),e("a",le,[t("http://arxiv.org/abs/2405.05548v1"),a(n)])]),ce,he,de,e("p",null,[ue,t(": "),e("a",ge,[t("http://arxiv.org/abs/2405.05469v1"),a(n)])]),pe,me,fe,ve,e("p",null,[be,t(": "),e("a",ye,[t("http://arxiv.org/abs/2405.05466v1"),a(n)])]),Le,we,_e,e("p",null,[ke,t(": "),e("a",xe,[t("http://arxiv.org/abs/2405.05465v1"),a(n)])]),Me,Ae,Te,e("p",null,[Ce,t(": "),e("a",Se,[t("http://arxiv.org/abs/2405.05444v1"),a(n)])]),Pe,ze,qe,e("p",null,[Ie,t(": "),e("a",We,[t("http://arxiv.org/abs/2405.05378v1"),a(n)])]),Ge,Ee,Re,e("p",null,[De,t(": "),e("a",He,[t("http://arxiv.org/abs/2405.05348v1"),a(n)])]),Ne,je,Oe,e("p",null,[Fe,t(": "),e("a",Ke,[t("http://arxiv.org/abs/2405.05345v1"),a(n)])]),Ve,Be,Je,e("p",null,[Ue,t(": "),e("a",Qe,[t("http://arxiv.org/abs/2405.05329v1"),a(n)])]),Ye,Ze,Xe,e("p",null,[$e,t(": "),e("a",et,[t("http://arxiv.org/abs/2405.05253v1"),a(n)])]),tt,nt,at,e("p",null,[it,t(": "),e("a",ot,[t("http://arxiv.org/abs/2405.05248v2"),a(n)])]),st,rt,lt,e("p",null,[ct,t(": "),e("a",ht,[t("http://arxiv.org/abs/2405.04819v1"),a(n)])]),dt,ut,gt,e("p",null,[pt,t(": "),e("a",mt,[t("http://arxiv.org/abs/2405.04798v1"),a(n)])]),ft,vt,bt,e("p",null,[yt,t(": "),e("a",Lt,[t("http://arxiv.org/abs/2405.04793v1"),a(n)])]),wt,_t,kt,e("p",null,[xt,t(": "),e("a",Mt,[t("http://arxiv.org/abs/2405.04727v1"),a(n)])]),At,Tt,Ct,St,e("p",null,[Pt,t(": "),e("a",zt,[t("http://arxiv.org/abs/2405.04656v1"),a(n)])]),qt,It,Wt,e("p",null,[Gt,t(": "),e("a",Et,[t("http://arxiv.org/abs/2405.04645v1"),a(n)])]),Rt,Dt,Ht,e("p",null,[Nt,t(": "),e("a",jt,[t("http://arxiv.org/abs/2405.04600v1"),a(n)])]),Ot,Ft,Kt,e("p",null,[Vt,t(": "),e("a",Bt,[t("http://arxiv.org/abs/2405.04532v1"),a(n)])]),Jt,Ut,Qt,e("p",null,[Yt,t(": "),e("a",Zt,[t("http://arxiv.org/abs/2405.04437v1"),a(n)])]),Xt,$t,en,e("p",null,[tn,t(": "),e("a",nn,[t("http://arxiv.org/abs/2405.04403v1"),a(n)])]),an,on,sn,e("p",null,[rn,t(": "),e("a",ln,[t("http://arxiv.org/abs/2405.04286v1"),a(n)])]),cn,hn,dn,e("p",null,[un,t(": "),e("a",gn,[t("http://arxiv.org/abs/2405.04282v1"),a(n)])]),pn,mn,fn,e("p",null,[vn,t(": "),e("a",bn,[t("http://arxiv.org/abs/2405.04215v1"),a(n)])]),yn,Ln,wn,e("p",null,[_n,t(": "),e("a",kn,[t("http://arxiv.org/abs/2405.03998v1"),a(n)])]),xn,Mn,An,Tn,e("p",null,[Cn,t(": "),e("a",Sn,[t("http://arxiv.org/abs/2405.03901v1"),a(n)])]),Pn,zn,qn,e("p",null,[In,t(": "),e("a",Wn,[t("http://arxiv.org/abs/2405.03862v1"),a(n)])]),Gn,En,Rn,e("p",null,[Dn,t(": "),e("a",Hn,[t("http://arxiv.org/abs/2405.03845v1"),a(n)])]),Nn,jn,On,e("p",null,[Fn,t(": "),e("a",Kn,[t("http://arxiv.org/abs/2405.03786v1"),a(n)])]),Vn,Bn,Jn,e("p",null,[Un,t(": "),e("a",Qn,[t("http://arxiv.org/abs/2405.03677v1"),a(n)])]),Yn,Zn,Xn,e("p",null,[$n,t(": "),e("a",ea,[t("http://arxiv.org/abs/2405.03654v2"),a(n)])]),ta,na,aa,e("p",null,[ia,t(": "),e("a",oa,[t("http://arxiv.org/abs/2405.03644v1"),a(n)])]),sa,ra,la,e("p",null,[ca,t(": "),e("a",ha,[t("http://arxiv.org/abs/2405.03637v1"),a(n)])]),da,ua,ga,e("p",null,[pa,t(": "),e("a",ma,[t("http://arxiv.org/abs/2405.03480v1"),a(n)])]),fa,va,ba,e("p",null,[ya,t(": "),e("a",La,[t("http://arxiv.org/abs/2405.03452v2"),a(n)])]),wa,_a,ka,e("p",null,[xa,t(": "),e("a",Ma,[t("http://arxiv.org/abs/2405.03279v2"),a(n)])]),Aa,Ta,Ca,e("p",null,[Sa,t(": "),e("a",Pa,[t("http://arxiv.org/abs/2405.03153v1"),a(n)])]),za,qa,Ia,e("p",null,[Wa,t(": "),e("a",Ga,[t("http://arxiv.org/abs/2405.03152v1"),a(n)])]),Ea,Ra,Da,e("p",null,[Ha,t(": "),e("a",Na,[t("http://arxiv.org/abs/2405.03146v2"),a(n)])]),ja,Oa,Fa,e("p",null,[Ka,t(": "),e("a",Va,[t("http://arxiv.org/abs/2405.03103v1"),a(n)])]),Ba])}const Za=i(c,[["render",Ja],["__file","LLM.html.vue"]]),Xa=JSON.parse('{"path":"/Arxiv/LLM/LLM.html","title":"LLM","lang":"en-US","frontmatter":{"description":"LLM 2024-05-09 Probing Multimodal LLMs as World Models for Driving Authors: Shiva Sreeram, Tsun-Hsuan Wang, Alaa Maalouf, Guy Rosman, Sertac Karaman, Daniela Rus Link: http://ar...","head":[["meta",{"property":"og:url","content":"https://www.shisanyi.vip/Arxiv/LLM/LLM.html"}],["meta",{"property":"og:site_name","content":"饰三姨——中古饰品"}],["meta",{"property":"og:title","content":"LLM"}],["meta",{"property":"og:description","content":"LLM 2024-05-09 Probing Multimodal LLMs as World Models for Driving Authors: Shiva Sreeram, Tsun-Hsuan Wang, Alaa Maalouf, Guy Rosman, Sertac Karaman, Daniela Rus Link: http://ar..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"LLM\\",\\"image\\":[\\"\\"],\\"dateModified\\":null,\\"author\\":[]}"]]},"headers":[{"level":2,"title":"2024-05-09","slug":"_2024-05-09","link":"#_2024-05-09","children":[]},{"level":2,"title":"2024-05-08","slug":"_2024-05-08","link":"#_2024-05-08","children":[]},{"level":2,"title":"2024-05-07","slug":"_2024-05-07","link":"#_2024-05-07","children":[]},{"level":2,"title":"2024-05-06","slug":"_2024-05-06","link":"#_2024-05-06","children":[]}],"git":{"updatedTime":null,"contributors":[]},"filePathRelative":"Arxiv/LLM/LLM.md","autoDesc":true,"excerpt":"\\n<h2>2024-05-09</h2>\\n<h4>Probing Multimodal LLMs as World Models for Driving</h4>\\n<p><strong>Authors</strong>: Shiva Sreeram, Tsun-Hsuan Wang, Alaa Maalouf, Guy Rosman, Sertac Karaman, Daniela Rus</p>\\n<p><strong>Link</strong>: <a href=\\"http://arxiv.org/abs/2405.05956v1\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">http://arxiv.org/abs/2405.05956v1</a></p>"}');export{Za as comp,Xa as data};
